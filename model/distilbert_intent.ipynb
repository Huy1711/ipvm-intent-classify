{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "label_to_id_dict = {\n",
    "    \"general_information\": 0,\n",
    "    \"account_help\": 1,\n",
    "    \"troubleshoot_product\": 2,\n",
    "    \"lookup_report\": 3,\n",
    "    \"lookup_person\": 4,\n",
    "}\n",
    "\n",
    "id_to_label_dict = {\n",
    "    0: \"general_information\",\n",
    "    1: \"account_help\",\n",
    "    2: \"troubleshoot_product\",\n",
    "    3: \"lookup_report\",\n",
    "    4: \"lookup_person\",\n",
    "}\n",
    "\n",
    "def load_dataset(file):\n",
    "    data, dataset = [], []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    for sent in data:\n",
    "        dataset.append(\n",
    "            {\n",
    "                \"text\": sent[\"text\"], \n",
    "                \"label\": label_to_id_dict[sent[\"label\"]],\n",
    "            }\n",
    "        )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"../eda/7k_sentences_train.jsonl\")\n",
    "eval_data = load_dataset(\"../eda/7k_sentences_eval.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def compute_accuracy(predictions, labels):\n",
    "#     acc_preds = 0\n",
    "#     for pred, label in zip(predictions, labels):\n",
    "#         if pred == label:\n",
    "#             acc_preds += 1\n",
    "#     return round(acc_preds / len(predictions), 4)\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return compute_accuracy(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        text = data[\"text\"]\n",
    "        label = data[\"label\"]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512\n",
    "training_set = TextClassificationDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = TextClassificationDataset(eval_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for data in tqdm(training_loader):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if i%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    i += 1\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41c083bdf2c4533a801b52cf5791f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.3291453719139099\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "Training Loss per 5000 steps: 0.3398304432630539\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "Training Loss per 5000 steps: 0.5230689148108164\n",
      "Training Accuracy per 5000 steps: 91.66666666666667\n",
      "Training Loss per 5000 steps: 0.6216069832444191\n",
      "Training Accuracy per 5000 steps: 87.5\n",
      "Training Loss per 5000 steps: 0.5631775975227356\n",
      "Training Accuracy per 5000 steps: 90.0\n",
      "Training Loss per 5000 steps: 0.5144612739483515\n",
      "Training Accuracy per 5000 steps: 91.66666666666667\n",
      "Training Loss per 5000 steps: 0.48559769136565073\n",
      "Training Accuracy per 5000 steps: 92.85714285714286\n",
      "Training Loss per 5000 steps: 0.5266474820673466\n",
      "Training Accuracy per 5000 steps: 90.625\n",
      "Training Loss per 5000 steps: 0.6487840579615699\n",
      "Training Accuracy per 5000 steps: 86.11111111111111\n",
      "Training Loss per 5000 steps: 0.6088391542434692\n",
      "Training Accuracy per 5000 steps: 87.5\n",
      "Training Loss per 5000 steps: 0.6307698054747148\n",
      "Training Accuracy per 5000 steps: 86.36363636363636\n",
      "Training Loss per 5000 steps: 0.6024209707975388\n",
      "Training Accuracy per 5000 steps: 87.5\n",
      "Training Loss per 5000 steps: 0.5724018204670686\n",
      "Training Accuracy per 5000 steps: 88.46153846153847\n",
      "Training Loss per 5000 steps: 0.5482011213898659\n",
      "Training Accuracy per 5000 steps: 89.28571428571429\n",
      "Training Loss per 5000 steps: 0.5657593995332718\n",
      "Training Accuracy per 5000 steps: 88.33333333333333\n",
      "Training Loss per 5000 steps: 0.5458351681008935\n",
      "Training Accuracy per 5000 steps: 89.0625\n",
      "Training Loss per 5000 steps: 0.5666438455090803\n",
      "Training Accuracy per 5000 steps: 88.23529411764706\n",
      "Training Loss per 5000 steps: 0.5456467775834931\n",
      "Training Accuracy per 5000 steps: 88.88888888888889\n",
      "Training Loss per 5000 steps: 0.5278433694651252\n",
      "Training Accuracy per 5000 steps: 89.47368421052632\n",
      "Training Loss per 5000 steps: 0.5107503049075603\n",
      "Training Accuracy per 5000 steps: 90.0\n",
      "Training Loss per 5000 steps: 0.49651224201633815\n",
      "Training Accuracy per 5000 steps: 90.47619047619048\n",
      "Training Loss per 5000 steps: 0.5091211599382487\n",
      "Training Accuracy per 5000 steps: 89.77272727272727\n",
      "Training Loss per 5000 steps: 0.5636323139719341\n",
      "Training Accuracy per 5000 steps: 88.04347826086956\n",
      "Training Loss per 5000 steps: 0.5485504896690448\n",
      "Training Accuracy per 5000 steps: 88.54166666666667\n",
      "Training Loss per 5000 steps: 0.5337914896011352\n",
      "Training Accuracy per 5000 steps: 89.0\n",
      "Training Loss per 5000 steps: 0.5449560651412377\n",
      "Training Accuracy per 5000 steps: 88.46153846153847\n",
      "Training Loss per 5000 steps: 0.5325289247212587\n",
      "Training Accuracy per 5000 steps: 88.88888888888889\n",
      "Training Loss per 5000 steps: 0.5401155427098274\n",
      "Training Accuracy per 5000 steps: 88.39285714285714\n",
      "Training Loss per 5000 steps: 0.5279807412418825\n",
      "Training Accuracy per 5000 steps: 88.79310344827586\n",
      "Training Loss per 5000 steps: 0.5358858769138654\n",
      "Training Accuracy per 5000 steps: 88.33333333333333\n",
      "Training Loss per 5000 steps: 0.5484678980804258\n",
      "Training Accuracy per 5000 steps: 87.90322580645162\n",
      "Training Loss per 5000 steps: 0.582266338635236\n",
      "Training Accuracy per 5000 steps: 86.71875\n",
      "Training Loss per 5000 steps: 0.5692873506835012\n",
      "Training Accuracy per 5000 steps: 87.12121212121212\n",
      "Training Loss per 5000 steps: 0.5572577951585546\n",
      "Training Accuracy per 5000 steps: 87.5\n",
      "Training Loss per 5000 steps: 0.5883167598928724\n",
      "Training Accuracy per 5000 steps: 86.42857142857143\n",
      "Training Loss per 5000 steps: 0.5986750448743502\n",
      "Training Accuracy per 5000 steps: 86.11111111111111\n",
      "Training Loss per 5000 steps: 0.607763395921604\n",
      "Training Accuracy per 5000 steps: 85.8108108108108\n",
      "Training Loss per 5000 steps: 0.595704094751885\n",
      "Training Accuracy per 5000 steps: 86.1842105263158\n",
      "Training Loss per 5000 steps: 0.6011783728996912\n",
      "Training Accuracy per 5000 steps: 85.8974358974359\n",
      "Training Loss per 5000 steps: 0.5903036538511515\n",
      "Training Accuracy per 5000 steps: 86.25\n",
      "Training Loss per 5000 steps: 0.581206343159443\n",
      "Training Accuracy per 5000 steps: 86.58536585365853\n",
      "Training Loss per 5000 steps: 0.5824143428887639\n",
      "Training Accuracy per 5000 steps: 86.30952380952381\n",
      "Training Loss per 5000 steps: 0.5826735700978789\n",
      "Training Accuracy per 5000 steps: 86.04651162790698\n",
      "Training Loss per 5000 steps: 0.5910148292102597\n",
      "Training Accuracy per 5000 steps: 85.79545454545455\n",
      "Training Loss per 5000 steps: 0.6198873675531811\n",
      "Training Accuracy per 5000 steps: 85.0\n",
      "Training Loss per 5000 steps: 0.6098164984065554\n",
      "Training Accuracy per 5000 steps: 85.32608695652173\n",
      "Training Loss per 5000 steps: 0.6125689126709675\n",
      "Training Accuracy per 5000 steps: 85.1063829787234\n",
      "Training Loss per 5000 steps: 0.6029690867289901\n",
      "Training Accuracy per 5000 steps: 85.41666666666667\n",
      "Training Loss per 5000 steps: 0.6030527782075259\n",
      "Training Accuracy per 5000 steps: 85.20408163265306\n",
      "Training Loss per 5000 steps: 0.5941889211535454\n",
      "Training Accuracy per 5000 steps: 85.5\n",
      "Training Loss per 5000 steps: 0.5856577114147299\n",
      "Training Accuracy per 5000 steps: 85.7843137254902\n",
      "Training Loss per 5000 steps: 0.5779911890052832\n",
      "Training Accuracy per 5000 steps: 86.0576923076923\n",
      "Training Loss per 5000 steps: 0.5704868579248212\n",
      "Training Accuracy per 5000 steps: 86.32075471698113\n",
      "Training Loss per 5000 steps: 0.5630333244248673\n",
      "Training Accuracy per 5000 steps: 86.57407407407408\n",
      "Training Loss per 5000 steps: 0.5617485580119219\n",
      "Training Accuracy per 5000 steps: 86.36363636363636\n",
      "Training Loss per 5000 steps: 0.5677311604044267\n",
      "Training Accuracy per 5000 steps: 86.16071428571429\n",
      "Training Loss per 5000 steps: 0.560489294560332\n",
      "Training Accuracy per 5000 steps: 86.40350877192982\n",
      "Training Loss per 5000 steps: 0.5687116114743824\n",
      "Training Accuracy per 5000 steps: 86.20689655172414\n",
      "Training Loss per 5000 steps: 0.5624257020526013\n",
      "Training Accuracy per 5000 steps: 86.44067796610169\n",
      "Training Loss per 5000 steps: 0.5772158461312453\n",
      "Training Accuracy per 5000 steps: 85.83333333333333\n",
      "Training Loss per 5000 steps: 0.5705193010021429\n",
      "Training Accuracy per 5000 steps: 86.06557377049181\n",
      "Training Loss per 5000 steps: 0.5639567858269138\n",
      "Training Accuracy per 5000 steps: 86.29032258064517\n",
      "Training Loss per 5000 steps: 0.5577338113198205\n",
      "Training Accuracy per 5000 steps: 86.5079365079365\n",
      "Training Loss per 5000 steps: 0.5518512348644435\n",
      "Training Accuracy per 5000 steps: 86.71875\n",
      "Training Loss per 5000 steps: 0.5657834103474251\n",
      "Training Accuracy per 5000 steps: 86.15384615384616\n",
      "Training Loss per 5000 steps: 0.570870554356864\n",
      "Training Accuracy per 5000 steps: 85.98484848484848\n",
      "Training Loss per 5000 steps: 0.5725869176103108\n",
      "Training Accuracy per 5000 steps: 85.82089552238806\n",
      "Training Loss per 5000 steps: 0.5669171358732616\n",
      "Training Accuracy per 5000 steps: 86.02941176470588\n",
      "Training Loss per 5000 steps: 0.561148806758549\n",
      "Training Accuracy per 5000 steps: 86.23188405797102\n",
      "Training Loss per 5000 steps: 0.568050068616867\n",
      "Training Accuracy per 5000 steps: 86.07142857142857\n",
      "Training Loss per 5000 steps: 0.5621957791523194\n",
      "Training Accuracy per 5000 steps: 86.26760563380282\n",
      "Training Loss per 5000 steps: 0.5651982588072618\n",
      "Training Accuracy per 5000 steps: 86.11111111111111\n",
      "Training Loss per 5000 steps: 0.5601774927687971\n",
      "Training Accuracy per 5000 steps: 86.3013698630137\n",
      "Training Loss per 5000 steps: 0.5550527097405614\n",
      "Training Accuracy per 5000 steps: 86.48648648648648\n",
      "Training Loss per 5000 steps: 0.5661516420046488\n",
      "Training Accuracy per 5000 steps: 86.0\n",
      "Training Loss per 5000 steps: 0.5610396975749418\n",
      "Training Accuracy per 5000 steps: 86.1842105263158\n",
      "Training Loss per 5000 steps: 0.5683710687346273\n",
      "Training Accuracy per 5000 steps: 85.71428571428571\n",
      "Training Loss per 5000 steps: 0.5819856497721795\n",
      "Training Accuracy per 5000 steps: 85.25641025641026\n",
      "Training Loss per 5000 steps: 0.5876341805427889\n",
      "Training Accuracy per 5000 steps: 85.12658227848101\n",
      "Training Loss per 5000 steps: 0.5820934694260359\n",
      "Training Accuracy per 5000 steps: 85.3125\n",
      "Training Loss per 5000 steps: 0.5986902724813532\n",
      "Training Accuracy per 5000 steps: 84.87654320987654\n",
      "Training Loss per 5000 steps: 0.593318764938087\n",
      "Training Accuracy per 5000 steps: 85.0609756097561\n",
      "Training Loss per 5000 steps: 0.595733562327293\n",
      "Training Accuracy per 5000 steps: 84.93975903614458\n",
      "Training Loss per 5000 steps: 0.5905970407738572\n",
      "Training Accuracy per 5000 steps: 85.11904761904762\n",
      "Training Loss per 5000 steps: 0.601606781517758\n",
      "Training Accuracy per 5000 steps: 84.70588235294117\n",
      "Training Loss per 5000 steps: 0.5967661598047544\n",
      "Training Accuracy per 5000 steps: 84.88372093023256\n",
      "Training Loss per 5000 steps: 0.602944280738118\n",
      "Training Accuracy per 5000 steps: 84.77011494252874\n",
      "Training Loss per 5000 steps: 0.6041300755671479\n",
      "Training Accuracy per 5000 steps: 84.6590909090909\n",
      "Training Loss per 5000 steps: 0.6120069453890404\n",
      "Training Accuracy per 5000 steps: 84.26966292134831\n",
      "Training Loss per 5000 steps: 0.6074530235595174\n",
      "Training Accuracy per 5000 steps: 84.44444444444444\n",
      "Training Loss per 5000 steps: 0.6033643364578813\n",
      "Training Accuracy per 5000 steps: 84.61538461538461\n",
      "Training Loss per 5000 steps: 0.5989236611387004\n",
      "Training Accuracy per 5000 steps: 84.78260869565217\n",
      "Training Loss per 5000 steps: 0.5994427543814465\n",
      "Training Accuracy per 5000 steps: 84.6774193548387\n",
      "Training Loss per 5000 steps: 0.6097533201917689\n",
      "Training Accuracy per 5000 steps: 84.30851063829788\n",
      "Training Loss per 5000 steps: 0.6054744791043433\n",
      "Training Accuracy per 5000 steps: 84.47368421052632\n",
      "Training Loss per 5000 steps: 0.6019285124105712\n",
      "Training Accuracy per 5000 steps: 84.63541666666667\n",
      "Training Loss per 5000 steps: 0.6015535717465214\n",
      "Training Accuracy per 5000 steps: 84.5360824742268\n",
      "Training Loss per 5000 steps: 0.5996223783918789\n",
      "Training Accuracy per 5000 steps: 84.6938775510204\n",
      "Training Loss per 5000 steps: 0.5974217705955409\n",
      "Training Accuracy per 5000 steps: 84.84848484848484\n",
      "Training Loss per 5000 steps: 0.5936151152849197\n",
      "Training Accuracy per 5000 steps: 85.0\n",
      "Training Loss per 5000 steps: 0.5894516895313074\n",
      "Training Accuracy per 5000 steps: 85.14851485148515\n",
      "Training Loss per 5000 steps: 0.5908736025585848\n",
      "Training Accuracy per 5000 steps: 85.04901960784314\n",
      "Training Loss per 5000 steps: 0.5949764726231399\n",
      "Training Accuracy per 5000 steps: 84.95145631067962\n",
      "Training Loss per 5000 steps: 0.5967122900944489\n",
      "Training Accuracy per 5000 steps: 84.85576923076923\n",
      "Training Loss per 5000 steps: 0.5994120268594652\n",
      "Training Accuracy per 5000 steps: 84.76190476190476\n",
      "Training Loss per 5000 steps: 0.6023106282612063\n",
      "Training Accuracy per 5000 steps: 84.43396226415095\n",
      "Training Loss per 5000 steps: 0.5982788531022651\n",
      "Training Accuracy per 5000 steps: 84.57943925233644\n",
      "Training Loss per 5000 steps: 0.5942792795874454\n",
      "Training Accuracy per 5000 steps: 84.72222222222223\n",
      "Training Loss per 5000 steps: 0.5904879360844236\n",
      "Training Accuracy per 5000 steps: 84.86238532110092\n",
      "Training Loss per 5000 steps: 0.593484802544117\n",
      "Training Accuracy per 5000 steps: 84.77272727272727\n",
      "Training Loss per 5000 steps: 0.6006497583679251\n",
      "Training Accuracy per 5000 steps: 84.68468468468468\n",
      "Training Loss per 5000 steps: 0.5967852539782014\n",
      "Training Accuracy per 5000 steps: 84.82142857142857\n",
      "Training Loss per 5000 steps: 0.5979096955430191\n",
      "Training Accuracy per 5000 steps: 84.73451327433628\n",
      "Training Loss per 5000 steps: 0.5993001275418097\n",
      "Training Accuracy per 5000 steps: 84.64912280701755\n",
      "Training Loss per 5000 steps: 0.5955315912547319\n",
      "Training Accuracy per 5000 steps: 84.78260869565217\n",
      "Training Loss per 5000 steps: 0.6010329478002828\n",
      "Training Accuracy per 5000 steps: 84.69827586206897\n",
      "Training Loss per 5000 steps: 0.5992380032936732\n",
      "Training Accuracy per 5000 steps: 84.82905982905983\n",
      "Training Loss per 5000 steps: 0.5953969486941726\n",
      "Training Accuracy per 5000 steps: 84.95762711864407\n",
      "Training Loss per 5000 steps: 0.6020829393332746\n",
      "Training Accuracy per 5000 steps: 84.66386554621849\n",
      "Training Loss per 5000 steps: 0.5985879941533009\n",
      "Training Accuracy per 5000 steps: 84.79166666666667\n",
      "Training Loss per 5000 steps: 0.59500161342877\n",
      "Training Accuracy per 5000 steps: 84.91735537190083\n",
      "Training Loss per 5000 steps: 0.5928072249058818\n",
      "Training Accuracy per 5000 steps: 85.04098360655738\n",
      "Training Loss per 5000 steps: 0.58915794743755\n",
      "Training Accuracy per 5000 steps: 85.16260162601625\n",
      "Training Loss per 5000 steps: 0.5975750595811875\n",
      "Training Accuracy per 5000 steps: 84.87903225806451\n",
      "Training Loss per 5000 steps: 0.5939383788108825\n",
      "Training Accuracy per 5000 steps: 85.0\n",
      "Training Loss per 5000 steps: 0.5902588630006427\n",
      "Training Accuracy per 5000 steps: 85.11904761904762\n",
      "Training Loss per 5000 steps: 0.5867681928037658\n",
      "Training Accuracy per 5000 steps: 85.23622047244095\n",
      "Training Loss per 5000 steps: 0.5835374023299664\n",
      "Training Accuracy per 5000 steps: 85.3515625\n",
      "Training Loss per 5000 steps: 0.5801443790280542\n",
      "Training Accuracy per 5000 steps: 85.46511627906976\n",
      "Training Loss per 5000 steps: 0.5801369708317977\n",
      "Training Accuracy per 5000 steps: 85.38461538461539\n",
      "Training Loss per 5000 steps: 0.586065432042566\n",
      "Training Accuracy per 5000 steps: 85.1145038167939\n",
      "Training Loss per 5000 steps: 0.5852790488438173\n",
      "Training Accuracy per 5000 steps: 85.03787878787878\n",
      "Training Loss per 5000 steps: 0.5818124488556295\n",
      "Training Accuracy per 5000 steps: 85.15037593984962\n",
      "Training Loss per 5000 steps: 0.5798403363841683\n",
      "Training Accuracy per 5000 steps: 85.26119402985074\n",
      "Training Loss per 5000 steps: 0.5765904781994996\n",
      "Training Accuracy per 5000 steps: 85.37037037037037\n",
      "Training Loss per 5000 steps: 0.5733982337748303\n",
      "Training Accuracy per 5000 steps: 85.4779411764706\n",
      "Training Loss per 5000 steps: 0.5702671787164507\n",
      "Training Accuracy per 5000 steps: 85.58394160583941\n",
      "Training Loss per 5000 steps: 0.5669827551405499\n",
      "Training Accuracy per 5000 steps: 85.68840579710145\n",
      "Training Loss per 5000 steps: 0.5640506108137343\n",
      "Training Accuracy per 5000 steps: 85.79136690647482\n",
      "Training Loss per 5000 steps: 0.5608843856624195\n",
      "Training Accuracy per 5000 steps: 85.89285714285714\n",
      "Training Loss per 5000 steps: 0.5577564436598872\n",
      "Training Accuracy per 5000 steps: 85.99290780141844\n",
      "Training Loss per 5000 steps: 0.5546200809029627\n",
      "Training Accuracy per 5000 steps: 86.09154929577464\n",
      "Training Loss per 5000 steps: 0.5574513903552002\n",
      "Training Accuracy per 5000 steps: 86.01398601398601\n",
      "Training Loss per 5000 steps: 0.5579728008661833\n",
      "Training Accuracy per 5000 steps: 85.9375\n",
      "Training Loss per 5000 steps: 0.5549851849675178\n",
      "Training Accuracy per 5000 steps: 86.03448275862068\n",
      "Training Loss per 5000 steps: 0.5519323370942514\n",
      "Training Accuracy per 5000 steps: 86.13013698630137\n",
      "Training Loss per 5000 steps: 0.5490130286942534\n",
      "Training Accuracy per 5000 steps: 86.22448979591837\n",
      "Training Loss per 5000 steps: 0.5459286205067828\n",
      "Training Accuracy per 5000 steps: 86.31756756756756\n",
      "Training Loss per 5000 steps: 0.543633377892059\n",
      "Training Accuracy per 5000 steps: 86.40939597315436\n",
      "Training Loss per 5000 steps: 0.54567827830712\n",
      "Training Accuracy per 5000 steps: 86.33333333333333\n",
      "Training Loss per 5000 steps: 0.5477263699501556\n",
      "Training Accuracy per 5000 steps: 86.25827814569537\n",
      "Training Loss per 5000 steps: 0.5499859202261034\n",
      "Training Accuracy per 5000 steps: 86.1842105263158\n",
      "Training Loss per 5000 steps: 0.5472052563834034\n",
      "Training Accuracy per 5000 steps: 86.27450980392157\n",
      "Training Loss per 5000 steps: 0.550654546684259\n",
      "Training Accuracy per 5000 steps: 86.2012987012987\n",
      "Training Loss per 5000 steps: 0.5521108322566556\n",
      "Training Accuracy per 5000 steps: 86.12903225806451\n",
      "Training Loss per 5000 steps: 0.5613630057718509\n",
      "Training Accuracy per 5000 steps: 85.73717948717949\n",
      "Training Loss per 5000 steps: 0.5596888565523609\n",
      "Training Accuracy per 5000 steps: 85.828025477707\n",
      "Training Loss per 5000 steps: 0.558955017807363\n",
      "Training Accuracy per 5000 steps: 85.75949367088607\n",
      "Training Loss per 5000 steps: 0.5562523085178819\n",
      "Training Accuracy per 5000 steps: 85.84905660377359\n",
      "Training Loss per 5000 steps: 0.5535142257809639\n",
      "Training Accuracy per 5000 steps: 85.9375\n",
      "Training Loss per 5000 steps: 0.5513710998785422\n",
      "Training Accuracy per 5000 steps: 86.0248447204969\n",
      "Training Loss per 5000 steps: 0.5492572891123501\n",
      "Training Accuracy per 5000 steps: 86.11111111111111\n",
      "Training Loss per 5000 steps: 0.5466091171181275\n",
      "Training Accuracy per 5000 steps: 86.1963190184049\n",
      "Training Loss per 5000 steps: 0.5440774137472234\n",
      "Training Accuracy per 5000 steps: 86.28048780487805\n",
      "Training Loss per 5000 steps: 0.5513978776606646\n",
      "Training Accuracy per 5000 steps: 86.06060606060606\n",
      "Training Loss per 5000 steps: 0.5488975700664233\n",
      "Training Accuracy per 5000 steps: 86.144578313253\n",
      "Training Loss per 5000 steps: 0.5465092709856832\n",
      "Training Accuracy per 5000 steps: 86.22754491017965\n",
      "Training Loss per 5000 steps: 0.5440433852019764\n",
      "Training Accuracy per 5000 steps: 86.30952380952381\n",
      "Training Loss per 5000 steps: 0.5436946241813299\n",
      "Training Accuracy per 5000 steps: 86.24260355029585\n",
      "Training Loss per 5000 steps: 0.542591792345047\n",
      "Training Accuracy per 5000 steps: 86.17647058823529\n",
      "Training Loss per 5000 steps: 0.5401305355000914\n",
      "Training Accuracy per 5000 steps: 86.25730994152046\n",
      "Training Loss per 5000 steps: 0.5378149176926114\n",
      "Training Accuracy per 5000 steps: 86.33720930232558\n",
      "Training Loss per 5000 steps: 0.5390824107937731\n",
      "Training Accuracy per 5000 steps: 86.27167630057804\n",
      "Training Loss per 5000 steps: 0.5366970068593134\n",
      "Training Accuracy per 5000 steps: 86.35057471264368\n",
      "Training Loss per 5000 steps: 0.5410683482885361\n",
      "Training Accuracy per 5000 steps: 86.28571428571429\n",
      "Training Loss per 5000 steps: 0.5387511592866345\n",
      "Training Accuracy per 5000 steps: 86.36363636363636\n",
      "Training Loss per 5000 steps: 0.5389182615583226\n",
      "Training Accuracy per 5000 steps: 86.29943502824858\n",
      "Training Loss per 5000 steps: 0.5376470083098733\n",
      "Training Accuracy per 5000 steps: 86.37640449438203\n",
      "Training Loss per 5000 steps: 0.5367213443504365\n",
      "Training Accuracy per 5000 steps: 86.31284916201118\n",
      "Training Loss per 5000 steps: 0.5341544355369277\n",
      "Training Accuracy per 5000 steps: 86.38888888888889\n",
      "Training Loss per 5000 steps: 0.5317054250539995\n",
      "Training Accuracy per 5000 steps: 86.46408839779005\n",
      "Training Loss per 5000 steps: 0.5348900557792449\n",
      "Training Accuracy per 5000 steps: 86.4010989010989\n",
      "Training Loss per 5000 steps: 0.5324525604319703\n",
      "Training Accuracy per 5000 steps: 86.47540983606558\n",
      "Training Loss per 5000 steps: 0.5301653952540263\n",
      "Training Accuracy per 5000 steps: 86.54891304347827\n",
      "Training Loss per 5000 steps: 0.5300059904117842\n",
      "Training Accuracy per 5000 steps: 86.48648648648648\n",
      "Training Loss per 5000 steps: 0.5277184484706771\n",
      "Training Accuracy per 5000 steps: 86.55913978494624\n",
      "Training Loss per 5000 steps: 0.5290081650417119\n",
      "Training Accuracy per 5000 steps: 86.49732620320856\n",
      "Training Loss per 5000 steps: 0.5290088726088722\n",
      "Training Accuracy per 5000 steps: 86.43617021276596\n",
      "Training Loss per 5000 steps: 0.5299777807618575\n",
      "Training Accuracy per 5000 steps: 86.24338624338624\n",
      "Training Loss per 5000 steps: 0.5301424600183964\n",
      "Training Accuracy per 5000 steps: 86.1842105263158\n",
      "Training Loss per 5000 steps: 0.5280639783172083\n",
      "Training Accuracy per 5000 steps: 86.2565445026178\n",
      "Training Loss per 5000 steps: 0.5286320082765693\n",
      "Training Accuracy per 5000 steps: 86.19791666666667\n",
      "Training Loss per 5000 steps: 0.5270046358204259\n",
      "Training Accuracy per 5000 steps: 86.26943005181347\n",
      "Training Loss per 5000 steps: 0.5250369203997027\n",
      "Training Accuracy per 5000 steps: 86.34020618556701\n",
      "Training Loss per 5000 steps: 0.5289935672512421\n",
      "Training Accuracy per 5000 steps: 86.28205128205128\n",
      "Training Loss per 5000 steps: 0.5273452796117992\n",
      "Training Accuracy per 5000 steps: 86.35204081632654\n",
      "Training Loss per 5000 steps: 0.5257868232385157\n",
      "Training Accuracy per 5000 steps: 86.42131979695432\n",
      "Training Loss per 5000 steps: 0.5262742483179377\n",
      "Training Accuracy per 5000 steps: 86.36363636363636\n",
      "Training Loss per 5000 steps: 0.5243065301422498\n",
      "Training Accuracy per 5000 steps: 86.4321608040201\n",
      "Training Loss per 5000 steps: 0.5237895921990275\n",
      "Training Accuracy per 5000 steps: 86.375\n",
      "Training Loss per 5000 steps: 0.5219527071907153\n",
      "Training Accuracy per 5000 steps: 86.44278606965175\n",
      "Training Loss per 5000 steps: 0.5210768609575116\n",
      "Training Accuracy per 5000 steps: 86.38613861386139\n",
      "Training Loss per 5000 steps: 0.5223523107875744\n",
      "Training Accuracy per 5000 steps: 86.33004926108374\n",
      "Training Loss per 5000 steps: 0.5203058546518579\n",
      "Training Accuracy per 5000 steps: 86.3970588235294\n",
      "Training Loss per 5000 steps: 0.5207588180536177\n",
      "Training Accuracy per 5000 steps: 86.34146341463415\n",
      "Training Loss per 5000 steps: 0.5274202686924379\n",
      "Training Accuracy per 5000 steps: 86.16504854368932\n",
      "Training Loss per 5000 steps: 0.5260058829317922\n",
      "Training Accuracy per 5000 steps: 86.23188405797102\n",
      "Training Loss per 5000 steps: 0.5257618566974998\n",
      "Training Accuracy per 5000 steps: 86.17788461538461\n",
      "Training Loss per 5000 steps: 0.5237207451933309\n",
      "Training Accuracy per 5000 steps: 86.24401913875599\n",
      "Training Loss per 5000 steps: 0.5217095909374101\n",
      "Training Accuracy per 5000 steps: 86.30952380952381\n",
      "Training Loss per 5000 steps: 0.5198207662145108\n",
      "Training Accuracy per 5000 steps: 86.37440758293839\n",
      "Training Loss per 5000 steps: 0.5177730530289546\n",
      "Training Accuracy per 5000 steps: 86.43867924528301\n",
      "Training Loss per 5000 steps: 0.5160272382024867\n",
      "Training Accuracy per 5000 steps: 86.50234741784037\n",
      "Training Loss per 5000 steps: 0.5140976832013264\n",
      "Training Accuracy per 5000 steps: 86.56542056074767\n",
      "Training Loss per 5000 steps: 0.513365085596262\n",
      "Training Accuracy per 5000 steps: 86.51162790697674\n",
      "Training Loss per 5000 steps: 0.5114911205514714\n",
      "Training Accuracy per 5000 steps: 86.57407407407408\n",
      "Training Loss per 5000 steps: 0.5121728699602839\n",
      "Training Accuracy per 5000 steps: 86.52073732718894\n",
      "Training Loss per 5000 steps: 0.5116253582709426\n",
      "Training Accuracy per 5000 steps: 86.46788990825688\n",
      "Training Loss per 5000 steps: 0.5096104681219684\n",
      "Training Accuracy per 5000 steps: 86.5296803652968\n",
      "Training Loss per 5000 steps: 0.5076783616095781\n",
      "Training Accuracy per 5000 steps: 86.5909090909091\n",
      "Training Loss per 5000 steps: 0.5070909200682899\n",
      "Training Accuracy per 5000 steps: 86.53846153846153\n",
      "Training Loss per 5000 steps: 0.5051411675574543\n",
      "Training Accuracy per 5000 steps: 86.59909909909909\n",
      "Training Loss per 5000 steps: 0.5049491860807744\n",
      "Training Accuracy per 5000 steps: 86.54708520179372\n",
      "Training Loss per 5000 steps: 0.5030468765180558\n",
      "Training Accuracy per 5000 steps: 86.60714285714286\n",
      "Training Loss per 5000 steps: 0.501191053589185\n",
      "Training Accuracy per 5000 steps: 86.66666666666667\n",
      "Training Loss per 5000 steps: 0.5012510721530534\n",
      "Training Accuracy per 5000 steps: 86.61504424778761\n",
      "Training Loss per 5000 steps: 0.5004637836753534\n",
      "Training Accuracy per 5000 steps: 86.67400881057269\n",
      "Training Loss per 5000 steps: 0.498595497325847\n",
      "Training Accuracy per 5000 steps: 86.73245614035088\n",
      "Training Loss per 5000 steps: 0.49669933013260104\n",
      "Training Accuracy per 5000 steps: 86.79039301310044\n",
      "Training Loss per 5000 steps: 0.503980525744998\n",
      "Training Accuracy per 5000 steps: 86.6304347826087\n",
      "Training Loss per 5000 steps: 0.5021467546234916\n",
      "Training Accuracy per 5000 steps: 86.68831168831169\n",
      "Training Loss per 5000 steps: 0.5007147476205538\n",
      "Training Accuracy per 5000 steps: 86.74568965517241\n",
      "Training Loss per 5000 steps: 0.5002228492958863\n",
      "Training Accuracy per 5000 steps: 86.69527896995709\n",
      "Training Loss per 5000 steps: 0.5004407784495598\n",
      "Training Accuracy per 5000 steps: 86.64529914529915\n",
      "Training Loss per 5000 steps: 0.4995325555826755\n",
      "Training Accuracy per 5000 steps: 86.70212765957447\n",
      "Training Loss per 5000 steps: 0.4998040212539293\n",
      "Training Accuracy per 5000 steps: 86.65254237288136\n",
      "Training Loss per 5000 steps: 0.4987912060464988\n",
      "Training Accuracy per 5000 steps: 86.70886075949367\n",
      "Training Loss per 5000 steps: 0.49988157640234765\n",
      "Training Accuracy per 5000 steps: 86.65966386554622\n",
      "Training Loss per 5000 steps: 0.4983574147628441\n",
      "Training Accuracy per 5000 steps: 86.71548117154812\n",
      "Training Loss per 5000 steps: 0.4965534484324356\n",
      "Training Accuracy per 5000 steps: 86.77083333333333\n",
      "Training Loss per 5000 steps: 0.49491856194630696\n",
      "Training Accuracy per 5000 steps: 86.82572614107883\n",
      "Training Loss per 5000 steps: 0.493194278575911\n",
      "Training Accuracy per 5000 steps: 86.8801652892562\n",
      "Training Loss per 5000 steps: 0.49717826997179065\n",
      "Training Accuracy per 5000 steps: 86.72839506172839\n",
      "Training Loss per 5000 steps: 0.49947828979643644\n",
      "Training Accuracy per 5000 steps: 86.68032786885246\n",
      "Training Loss per 5000 steps: 0.4977012947505834\n",
      "Training Accuracy per 5000 steps: 86.73469387755102\n",
      "Training Loss per 5000 steps: 0.49589020089526487\n",
      "Training Accuracy per 5000 steps: 86.78861788617886\n",
      "Training Loss per 5000 steps: 0.49414318790923245\n",
      "Training Accuracy per 5000 steps: 86.84210526315789\n",
      "Training Loss per 5000 steps: 0.49402003606120426\n",
      "Training Accuracy per 5000 steps: 86.79435483870968\n",
      "Training Loss per 5000 steps: 0.492403862915604\n",
      "Training Accuracy per 5000 steps: 86.84738955823293\n",
      "Training Loss per 5000 steps: 0.4908762956261635\n",
      "Training Accuracy per 5000 steps: 86.9\n",
      "Training Loss per 5000 steps: 0.489224296909167\n",
      "Training Accuracy per 5000 steps: 86.95219123505976\n",
      "Training Loss per 5000 steps: 0.4876256525102589\n",
      "Training Accuracy per 5000 steps: 87.00396825396825\n",
      "Training Loss per 5000 steps: 0.48891798018113425\n",
      "Training Accuracy per 5000 steps: 86.95652173913044\n",
      "Training Loss per 5000 steps: 0.4872260302686551\n",
      "Training Accuracy per 5000 steps: 87.00787401574803\n",
      "Training Loss per 5000 steps: 0.4861784931637493\n",
      "Training Accuracy per 5000 steps: 87.05882352941177\n",
      "Training Loss per 5000 steps: 0.48448962725524325\n",
      "Training Accuracy per 5000 steps: 87.109375\n",
      "Training Loss per 5000 steps: 0.48287621817410226\n",
      "Training Accuracy per 5000 steps: 87.15953307392996\n",
      "Training Loss per 5000 steps: 0.48138992946104026\n",
      "Training Accuracy per 5000 steps: 87.20930232558139\n",
      "Training Loss per 5000 steps: 0.4801316290202058\n",
      "Training Accuracy per 5000 steps: 87.25868725868726\n",
      "Training Loss per 5000 steps: 0.4785741340225706\n",
      "Training Accuracy per 5000 steps: 87.3076923076923\n",
      "Training Loss per 5000 steps: 0.47690550808819776\n",
      "Training Accuracy per 5000 steps: 87.35632183908046\n",
      "Training Loss per 5000 steps: 0.47535549670116595\n",
      "Training Accuracy per 5000 steps: 87.40458015267176\n",
      "Training Loss per 5000 steps: 0.47378436658715567\n",
      "Training Accuracy per 5000 steps: 87.45247148288973\n",
      "Training Loss per 5000 steps: 0.4723343601252771\n",
      "Training Accuracy per 5000 steps: 87.5\n",
      "Training Loss per 5000 steps: 0.4708553320253795\n",
      "Training Accuracy per 5000 steps: 87.54716981132076\n",
      "Training Loss per 5000 steps: 0.4692860931522192\n",
      "Training Accuracy per 5000 steps: 87.59398496240601\n",
      "Training Loss per 5000 steps: 0.4704361241101549\n",
      "Training Accuracy per 5000 steps: 87.54681647940075\n",
      "Training Loss per 5000 steps: 0.4692543305373236\n",
      "Training Accuracy per 5000 steps: 87.59328358208955\n",
      "Training Loss per 5000 steps: 0.46843530698835184\n",
      "Training Accuracy per 5000 steps: 87.63940520446097\n",
      "Training Loss per 5000 steps: 0.467918944593381\n",
      "Training Accuracy per 5000 steps: 87.68518518518519\n",
      "Training Loss per 5000 steps: 0.46942858789734737\n",
      "Training Accuracy per 5000 steps: 87.63837638376384\n",
      "Training Loss per 5000 steps: 0.4683614297742572\n",
      "Training Accuracy per 5000 steps: 87.68382352941177\n",
      "Training Loss per 5000 steps: 0.4670363150045767\n",
      "Training Accuracy per 5000 steps: 87.72893772893772\n",
      "Training Loss per 5000 steps: 0.46701735221393353\n",
      "Training Accuracy per 5000 steps: 87.68248175182482\n",
      "Training Loss per 5000 steps: 0.4662816968424754\n",
      "Training Accuracy per 5000 steps: 87.63636363636364\n",
      "Training Loss per 5000 steps: 0.4650227371198328\n",
      "Training Accuracy per 5000 steps: 87.68115942028986\n",
      "Training Loss per 5000 steps: 0.46781404912202795\n",
      "Training Accuracy per 5000 steps: 87.63537906137184\n",
      "Training Loss per 5000 steps: 0.4665921633165303\n",
      "Training Accuracy per 5000 steps: 87.67985611510791\n",
      "Training Loss per 5000 steps: 0.46593996149397665\n",
      "Training Accuracy per 5000 steps: 87.72401433691756\n",
      "Training Loss per 5000 steps: 0.4656004097179643\n",
      "Training Accuracy per 5000 steps: 87.76785714285714\n",
      "Training Loss per 5000 steps: 0.4657465198683781\n",
      "Training Accuracy per 5000 steps: 87.72241992882562\n",
      "Training Loss per 5000 steps: 0.4642893681104513\n",
      "Training Accuracy per 5000 steps: 87.76595744680851\n",
      "Training Loss per 5000 steps: 0.46284076491166765\n",
      "Training Accuracy per 5000 steps: 87.80918727915194\n",
      "Training Loss per 5000 steps: 0.4620694403051281\n",
      "Training Accuracy per 5000 steps: 87.85211267605634\n",
      "Training Loss per 5000 steps: 0.4607508353888988\n",
      "Training Accuracy per 5000 steps: 87.89473684210526\n",
      "Training Loss per 5000 steps: 0.4623483329672705\n",
      "Training Accuracy per 5000 steps: 87.84965034965035\n",
      "Training Loss per 5000 steps: 0.46261498403414203\n",
      "Training Accuracy per 5000 steps: 87.8048780487805\n",
      "Training Loss per 5000 steps: 0.4620309277411757\n",
      "Training Accuracy per 5000 steps: 87.84722222222223\n",
      "Training Loss per 5000 steps: 0.46290072436512136\n",
      "Training Accuracy per 5000 steps: 87.80276816608996\n",
      "Training Loss per 5000 steps: 0.46148860658808\n",
      "Training Accuracy per 5000 steps: 87.84482758620689\n",
      "Training Loss per 5000 steps: 0.4610250677106921\n",
      "Training Accuracy per 5000 steps: 87.88659793814433\n",
      "Training Loss per 5000 steps: 0.4599253349899225\n",
      "Training Accuracy per 5000 steps: 87.92808219178082\n",
      "Training Loss per 5000 steps: 0.4585122782800588\n",
      "Training Accuracy per 5000 steps: 87.96928327645051\n",
      "Training Loss per 5000 steps: 0.4582965863982634\n",
      "Training Accuracy per 5000 steps: 87.92517006802721\n",
      "Training Loss per 5000 steps: 0.457144404859361\n",
      "Training Accuracy per 5000 steps: 87.96610169491525\n",
      "Training Loss per 5000 steps: 0.45617159615849723\n",
      "Training Accuracy per 5000 steps: 88.00675675675676\n",
      "Training Loss per 5000 steps: 0.4556202531136848\n",
      "Training Accuracy per 5000 steps: 87.96296296296296\n",
      "Training Loss per 5000 steps: 0.45435748050526886\n",
      "Training Accuracy per 5000 steps: 88.00335570469798\n",
      "Training Loss per 5000 steps: 0.4536720986873609\n",
      "Training Accuracy per 5000 steps: 88.04347826086956\n",
      "Training Loss per 5000 steps: 0.45264183105280004\n",
      "Training Accuracy per 5000 steps: 88.08333333333333\n",
      "Training Loss per 5000 steps: 0.45146929567982985\n",
      "Training Accuracy per 5000 steps: 88.12292358803987\n",
      "Training Loss per 5000 steps: 0.4501542221532752\n",
      "Training Accuracy per 5000 steps: 88.16225165562913\n",
      "Training Loss per 5000 steps: 0.44893642214953705\n",
      "Training Accuracy per 5000 steps: 88.20132013201321\n",
      "Training Loss per 5000 steps: 0.4475876444852666\n",
      "Training Accuracy per 5000 steps: 88.24013157894737\n",
      "Training Loss per 5000 steps: 0.44666744646478873\n",
      "Training Accuracy per 5000 steps: 88.27868852459017\n",
      "Training Loss per 5000 steps: 0.44534242055774514\n",
      "Training Accuracy per 5000 steps: 88.31699346405229\n",
      "Training Loss per 5000 steps: 0.4440191627301106\n",
      "Training Accuracy per 5000 steps: 88.35504885993485\n",
      "Training Loss per 5000 steps: 0.4427292451576947\n",
      "Training Accuracy per 5000 steps: 88.39285714285714\n",
      "Training Loss per 5000 steps: 0.4414163122381593\n",
      "Training Accuracy per 5000 steps: 88.43042071197411\n",
      "Training Loss per 5000 steps: 0.44153858379971594\n",
      "Training Accuracy per 5000 steps: 88.38709677419355\n",
      "Training Loss per 5000 steps: 0.44073571207247364\n",
      "Training Accuracy per 5000 steps: 88.42443729903538\n",
      "Training Loss per 5000 steps: 0.43942614336713004\n",
      "Training Accuracy per 5000 steps: 88.46153846153847\n",
      "Training Loss per 5000 steps: 0.4426113989549323\n",
      "Training Accuracy per 5000 steps: 88.33865814696486\n",
      "Training Loss per 5000 steps: 0.4416593766658549\n",
      "Training Accuracy per 5000 steps: 88.37579617834395\n",
      "Training Loss per 5000 steps: 0.44037694532483346\n",
      "Training Accuracy per 5000 steps: 88.41269841269842\n",
      "Training Loss per 5000 steps: 0.43911144638410476\n",
      "Training Accuracy per 5000 steps: 88.4493670886076\n",
      "Training Loss per 5000 steps: 0.4387325820416118\n",
      "Training Accuracy per 5000 steps: 88.40694006309148\n",
      "Training Loss per 5000 steps: 0.4384540004667434\n",
      "Training Accuracy per 5000 steps: 88.36477987421384\n",
      "Training Loss per 5000 steps: 0.43725588099102614\n",
      "Training Accuracy per 5000 steps: 88.4012539184953\n",
      "Training Loss per 5000 steps: 0.43647123979171737\n",
      "Training Accuracy per 5000 steps: 88.4375\n",
      "Training Loss per 5000 steps: 0.43571120940404146\n",
      "Training Accuracy per 5000 steps: 88.47352024922118\n",
      "Training Loss per 5000 steps: 0.4360560813097295\n",
      "Training Accuracy per 5000 steps: 88.43167701863354\n",
      "Training Loss per 5000 steps: 0.43526814198669267\n",
      "Training Accuracy per 5000 steps: 88.46749226006192\n",
      "Training Loss per 5000 steps: 0.4344208034353308\n",
      "Training Accuracy per 5000 steps: 88.50308641975309\n",
      "Training Loss per 5000 steps: 0.4331999439574205\n",
      "Training Accuracy per 5000 steps: 88.53846153846153\n",
      "Training Loss per 5000 steps: 0.43723627048447455\n",
      "Training Accuracy per 5000 steps: 88.420245398773\n",
      "Training Loss per 5000 steps: 0.43600923392568525\n",
      "Training Accuracy per 5000 steps: 88.45565749235475\n",
      "Training Loss per 5000 steps: 0.43529068088022677\n",
      "Training Accuracy per 5000 steps: 88.49085365853658\n",
      "Training Loss per 5000 steps: 0.4340808007718944\n",
      "Training Accuracy per 5000 steps: 88.5258358662614\n",
      "Training Loss per 5000 steps: 0.4339463853700595\n",
      "Training Accuracy per 5000 steps: 88.48484848484848\n",
      "Training Loss per 5000 steps: 0.43430556597756476\n",
      "Training Accuracy per 5000 steps: 88.4441087613293\n",
      "Training Loss per 5000 steps: 0.43467991451272764\n",
      "Training Accuracy per 5000 steps: 88.32831325301204\n",
      "Training Loss per 5000 steps: 0.4337449297756404\n",
      "Training Accuracy per 5000 steps: 88.36336336336336\n",
      "Training Loss per 5000 steps: 0.4330904922130222\n",
      "Training Accuracy per 5000 steps: 88.39820359281437\n",
      "Training Loss per 5000 steps: 0.4356736022367406\n",
      "Training Accuracy per 5000 steps: 88.35820895522389\n",
      "Training Loss per 5000 steps: 0.43589342290180777\n",
      "Training Accuracy per 5000 steps: 88.24404761904762\n",
      "Training Loss per 5000 steps: 0.4347341857651575\n",
      "Training Accuracy per 5000 steps: 88.27893175074183\n",
      "Training Loss per 5000 steps: 0.43357646393661314\n",
      "Training Accuracy per 5000 steps: 88.31360946745562\n",
      "Training Loss per 5000 steps: 0.4325349091010987\n",
      "Training Accuracy per 5000 steps: 88.34808259587021\n",
      "Training Loss per 5000 steps: 0.43544241532902506\n",
      "Training Accuracy per 5000 steps: 88.30882352941177\n",
      "Training Loss per 5000 steps: 0.43460106496805667\n",
      "Training Accuracy per 5000 steps: 88.34310850439883\n",
      "Training Loss per 5000 steps: 0.4343070920818208\n",
      "Training Accuracy per 5000 steps: 88.30409356725146\n",
      "Training Loss per 5000 steps: 0.43370538160378663\n",
      "Training Accuracy per 5000 steps: 88.33819241982508\n",
      "Training Loss per 5000 steps: 0.43276105265587916\n",
      "Training Accuracy per 5000 steps: 88.37209302325581\n",
      "Training Loss per 5000 steps: 0.4317581626060216\n",
      "Training Accuracy per 5000 steps: 88.40579710144928\n",
      "Training Loss per 5000 steps: 0.43070280987963167\n",
      "Training Accuracy per 5000 steps: 88.4393063583815\n",
      "Training Loss per 5000 steps: 0.43038919126824277\n",
      "Training Accuracy per 5000 steps: 88.40057636887607\n",
      "Training Loss per 5000 steps: 0.43101105501811054\n",
      "Training Accuracy per 5000 steps: 88.29022988505747\n",
      "Training Loss per 5000 steps: 0.4309200969522013\n",
      "Training Accuracy per 5000 steps: 88.32378223495702\n",
      "Training Loss per 5000 steps: 0.42984398142567704\n",
      "Training Accuracy per 5000 steps: 88.35714285714286\n",
      "Training Loss per 5000 steps: 0.4292560136874347\n",
      "Training Accuracy per 5000 steps: 88.3903133903134\n",
      "Training Loss per 5000 steps: 0.42816167569253594\n",
      "Training Accuracy per 5000 steps: 88.42329545454545\n",
      "Training Loss per 5000 steps: 0.4281533172106101\n",
      "Training Accuracy per 5000 steps: 88.38526912181302\n",
      "Training Loss per 5000 steps: 0.42705023943283466\n",
      "Training Accuracy per 5000 steps: 88.4180790960452\n",
      "Training Loss per 5000 steps: 0.4259867909298816\n",
      "Training Accuracy per 5000 steps: 88.45070422535211\n",
      "Training Loss per 5000 steps: 0.42492195316119474\n",
      "Training Accuracy per 5000 steps: 88.48314606741573\n",
      "Training Loss per 5000 steps: 0.42400273063979227\n",
      "Training Accuracy per 5000 steps: 88.51540616246498\n",
      "Training Loss per 5000 steps: 0.42293238008197126\n",
      "Training Accuracy per 5000 steps: 88.54748603351955\n",
      "Training Loss per 5000 steps: 0.42507239338788816\n",
      "Training Accuracy per 5000 steps: 88.50974930362116\n",
      "Training Loss per 5000 steps: 0.42401185235422517\n",
      "Training Accuracy per 5000 steps: 88.54166666666667\n",
      "Training Loss per 5000 steps: 0.422942721932466\n",
      "Training Accuracy per 5000 steps: 88.57340720221606\n",
      "Training Loss per 5000 steps: 0.42219175047431534\n",
      "Training Accuracy per 5000 steps: 88.60497237569061\n",
      "Training Loss per 5000 steps: 0.42145126486407497\n",
      "Training Accuracy per 5000 steps: 88.63636363636364\n",
      "Training Loss per 5000 steps: 0.4213864799527513\n",
      "Training Accuracy per 5000 steps: 88.5989010989011\n",
      "Training Loss per 5000 steps: 0.42073884029706865\n",
      "Training Accuracy per 5000 steps: 88.63013698630137\n",
      "Training Loss per 5000 steps: 0.4203446542772928\n",
      "Training Accuracy per 5000 steps: 88.66120218579235\n",
      "Training Loss per 5000 steps: 0.42205090803533224\n",
      "Training Accuracy per 5000 steps: 88.5558583106267\n",
      "Training Loss per 5000 steps: 0.42099442851284274\n",
      "Training Accuracy per 5000 steps: 88.58695652173913\n",
      "Training Loss per 5000 steps: 0.4200991903580624\n",
      "Training Accuracy per 5000 steps: 88.6178861788618\n",
      "Training Loss per 5000 steps: 0.4193292549937158\n",
      "Training Accuracy per 5000 steps: 88.64864864864865\n",
      "Training Loss per 5000 steps: 0.4182847810361906\n",
      "Training Accuracy per 5000 steps: 88.67924528301887\n",
      "Training Loss per 5000 steps: 0.4182232262426487\n",
      "Training Accuracy per 5000 steps: 88.64247311827957\n",
      "Training Loss per 5000 steps: 0.41906302510173965\n",
      "Training Accuracy per 5000 steps: 88.6058981233244\n",
      "Training Loss per 5000 steps: 0.41801937636645403\n",
      "Training Accuracy per 5000 steps: 88.63636363636364\n",
      "Training Loss per 5000 steps: 0.4170492490430673\n",
      "Training Accuracy per 5000 steps: 88.66666666666667\n",
      "Training Loss per 5000 steps: 0.41866250465960897\n",
      "Training Accuracy per 5000 steps: 88.63031914893617\n",
      "Training Loss per 5000 steps: 0.41813640319186435\n",
      "Training Accuracy per 5000 steps: 88.6604774535809\n",
      "Training Loss per 5000 steps: 0.41716144505947356\n",
      "Training Accuracy per 5000 steps: 88.69047619047619\n",
      "Training Loss per 5000 steps: 0.41790365677395724\n",
      "Training Accuracy per 5000 steps: 88.65435356200528\n",
      "Training Loss per 5000 steps: 0.41779274524826754\n",
      "Training Accuracy per 5000 steps: 88.61842105263158\n",
      "Training Loss per 5000 steps: 0.41705824258759266\n",
      "Training Accuracy per 5000 steps: 88.64829396325459\n",
      "Training Loss per 5000 steps: 0.4161499451202248\n",
      "Training Accuracy per 5000 steps: 88.67801047120419\n",
      "Training Loss per 5000 steps: 0.4154543808950768\n",
      "Training Accuracy per 5000 steps: 88.70757180156657\n",
      "Training Loss per 5000 steps: 0.41452377951160696\n",
      "Training Accuracy per 5000 steps: 88.73697916666667\n",
      "Training Loss per 5000 steps: 0.41387308613433466\n",
      "Training Accuracy per 5000 steps: 88.76623376623377\n",
      "Training Loss per 5000 steps: 0.41420510837837204\n",
      "Training Accuracy per 5000 steps: 88.73056994818653\n",
      "Training Loss per 5000 steps: 0.4134729613744935\n",
      "Training Accuracy per 5000 steps: 88.75968992248062\n",
      "Training Loss per 5000 steps: 0.41276877598126527\n",
      "Training Accuracy per 5000 steps: 88.78865979381443\n",
      "Training Loss per 5000 steps: 0.41184289879847924\n",
      "Training Accuracy per 5000 steps: 88.81748071979435\n",
      "Training Loss per 5000 steps: 0.4116931170225143\n",
      "Training Accuracy per 5000 steps: 88.84615384615384\n",
      "Training Loss per 5000 steps: 0.4121089093672955\n",
      "Training Accuracy per 5000 steps: 88.81074168797954\n",
      "Training Loss per 5000 steps: 0.41131842408177194\n",
      "Training Accuracy per 5000 steps: 88.83928571428571\n",
      "Training Loss per 5000 steps: 0.4105163152512097\n",
      "Training Accuracy per 5000 steps: 88.8676844783715\n",
      "Training Loss per 5000 steps: 0.40964128761575913\n",
      "Training Accuracy per 5000 steps: 88.89593908629442\n",
      "Training Loss per 5000 steps: 0.4090427667279787\n",
      "Training Accuracy per 5000 steps: 88.92405063291139\n",
      "Training Loss per 5000 steps: 0.408622836712936\n",
      "Training Accuracy per 5000 steps: 88.95202020202021\n",
      "Training Loss per 5000 steps: 0.4083812896265491\n",
      "Training Accuracy per 5000 steps: 88.97984886649874\n",
      "Training Loss per 5000 steps: 0.4079109822310994\n",
      "Training Accuracy per 5000 steps: 89.00753768844221\n",
      "Training Loss per 5000 steps: 0.4069616319074816\n",
      "Training Accuracy per 5000 steps: 89.03508771929825\n",
      "Training Loss per 5000 steps: 0.4068789141904563\n",
      "Training Accuracy per 5000 steps: 89.0\n",
      "Training Loss per 5000 steps: 0.4063455693498068\n",
      "Training Accuracy per 5000 steps: 89.02743142144638\n",
      "Training Loss per 5000 steps: 0.405463395213977\n",
      "Training Accuracy per 5000 steps: 89.05472636815921\n",
      "Training Loss per 5000 steps: 0.40454893052651925\n",
      "Training Accuracy per 5000 steps: 89.08188585607941\n",
      "Training Loss per 5000 steps: 0.4039639036659852\n",
      "Training Accuracy per 5000 steps: 89.10891089108911\n",
      "Training Loss per 5000 steps: 0.40906095767830625\n",
      "Training Accuracy per 5000 steps: 88.95061728395062\n",
      "Training Loss per 5000 steps: 0.40813000426797447\n",
      "Training Accuracy per 5000 steps: 88.97783251231527\n",
      "Training Loss per 5000 steps: 0.40774837429166133\n",
      "Training Accuracy per 5000 steps: 89.004914004914\n",
      "Training Loss per 5000 steps: 0.40705548661450547\n",
      "Training Accuracy per 5000 steps: 89.03186274509804\n",
      "Training Loss per 5000 steps: 0.4064361951461923\n",
      "Training Accuracy per 5000 steps: 89.05867970660147\n",
      "Training Loss per 5000 steps: 0.40861315392866365\n",
      "Training Accuracy per 5000 steps: 88.96341463414635\n",
      "Training Loss per 5000 steps: 0.40769572433655277\n",
      "Training Accuracy per 5000 steps: 88.99026763990268\n",
      "Training Loss per 5000 steps: 0.4067767091791679\n",
      "Training Accuracy per 5000 steps: 89.01699029126213\n",
      "Training Loss per 5000 steps: 0.405914608853673\n",
      "Training Accuracy per 5000 steps: 89.04358353510897\n",
      "Training Loss per 5000 steps: 0.4052312284177124\n",
      "Training Accuracy per 5000 steps: 89.07004830917874\n",
      "Training Loss per 5000 steps: 0.40495693698435664\n",
      "Training Accuracy per 5000 steps: 89.09638554216868\n",
      "Training Loss per 5000 steps: 0.40414020942541984\n",
      "Training Accuracy per 5000 steps: 89.12259615384616\n",
      "Training Loss per 5000 steps: 0.40370752796870196\n",
      "Training Accuracy per 5000 steps: 89.14868105515588\n",
      "Training Loss per 5000 steps: 0.403484883388442\n",
      "Training Accuracy per 5000 steps: 89.11483253588517\n",
      "Training Loss per 5000 steps: 0.40405014239536124\n",
      "Training Accuracy per 5000 steps: 89.08114558472553\n",
      "Training Loss per 5000 steps: 0.4031684175621541\n",
      "Training Accuracy per 5000 steps: 89.10714285714286\n",
      "Training Loss per 5000 steps: 0.4024275451004859\n",
      "Training Accuracy per 5000 steps: 89.13301662707839\n",
      "Training Loss per 5000 steps: 0.40155717499677746\n",
      "Training Accuracy per 5000 steps: 89.15876777251185\n",
      "Training Loss per 5000 steps: 0.40087323958586996\n",
      "Training Accuracy per 5000 steps: 89.18439716312056\n",
      "Training Loss per 5000 steps: 0.4000022932315983\n",
      "Training Accuracy per 5000 steps: 89.20990566037736\n",
      "Training Loss per 5000 steps: 0.40101913965361957\n",
      "Training Accuracy per 5000 steps: 89.17647058823529\n",
      "Training Loss per 5000 steps: 0.4001912009626837\n",
      "Training Accuracy per 5000 steps: 89.2018779342723\n",
      "Training Loss per 5000 steps: 0.3993218981855019\n",
      "Training Accuracy per 5000 steps: 89.22716627634661\n",
      "Training Loss per 5000 steps: 0.39847346591528193\n",
      "Training Accuracy per 5000 steps: 89.25233644859813\n",
      "Training Loss per 5000 steps: 0.398578316403123\n",
      "Training Accuracy per 5000 steps: 89.21911421911422\n",
      "Training Loss per 5000 steps: 0.39788156669413627\n",
      "Training Accuracy per 5000 steps: 89.24418604651163\n",
      "Training Loss per 5000 steps: 0.39973147871651254\n",
      "Training Accuracy per 5000 steps: 89.21113689095128\n",
      "Training Loss per 5000 steps: 0.39888602464580564\n",
      "Training Accuracy per 5000 steps: 89.23611111111111\n",
      "Training Loss per 5000 steps: 0.39808457596421104\n",
      "Training Accuracy per 5000 steps: 89.26096997690532\n",
      "Training Loss per 5000 steps: 0.39726422416810203\n",
      "Training Accuracy per 5000 steps: 89.28571428571429\n",
      "Training Loss per 5000 steps: 0.3966689619738823\n",
      "Training Accuracy per 5000 steps: 89.3103448275862\n",
      "Training Loss per 5000 steps: 0.39583157698000104\n",
      "Training Accuracy per 5000 steps: 89.3348623853211\n",
      "Training Loss per 5000 steps: 0.3950861364014839\n",
      "Training Accuracy per 5000 steps: 89.35926773455378\n",
      "Training Loss per 5000 steps: 0.3942609138097869\n",
      "Training Accuracy per 5000 steps: 89.38356164383562\n",
      "Training Loss per 5000 steps: 0.39358427704381915\n",
      "Training Accuracy per 5000 steps: 89.40774487471526\n",
      "Training Loss per 5000 steps: 0.39340215106494725\n",
      "Training Accuracy per 5000 steps: 89.375\n",
      "Training Loss per 5000 steps: 0.3926190299165695\n",
      "Training Accuracy per 5000 steps: 89.39909297052154\n",
      "Training Loss per 5000 steps: 0.3927736593646364\n",
      "Training Accuracy per 5000 steps: 89.36651583710407\n",
      "Training Loss per 5000 steps: 0.3919978435981691\n",
      "Training Accuracy per 5000 steps: 89.39051918735892\n",
      "Training Loss per 5000 steps: 0.3922935024382094\n",
      "Training Accuracy per 5000 steps: 89.35810810810811\n",
      "Training Loss per 5000 steps: 0.391474744656615\n",
      "Training Accuracy per 5000 steps: 89.38202247191012\n",
      "Training Loss per 5000 steps: 0.3908295374605888\n",
      "Training Accuracy per 5000 steps: 89.40582959641256\n",
      "Training Loss per 5000 steps: 0.3914739790703826\n",
      "Training Accuracy per 5000 steps: 89.37360178970917\n",
      "Training Loss per 5000 steps: 0.3906715250299645\n",
      "Training Accuracy per 5000 steps: 89.39732142857143\n",
      "Training Loss per 5000 steps: 0.3901870533583358\n",
      "Training Accuracy per 5000 steps: 89.42093541202672\n",
      "Training Loss per 5000 steps: 0.39039491124865083\n",
      "Training Accuracy per 5000 steps: 89.38888888888889\n",
      "Training Loss per 5000 steps: 0.38962569976113987\n",
      "Training Accuracy per 5000 steps: 89.41241685144124\n",
      "Training Loss per 5000 steps: 0.38893002595966764\n",
      "Training Accuracy per 5000 steps: 89.4358407079646\n",
      "Training Loss per 5000 steps: 0.38834429748981336\n",
      "Training Accuracy per 5000 steps: 89.45916114790288\n",
      "Training Loss per 5000 steps: 0.38774088041093774\n",
      "Training Accuracy per 5000 steps: 89.48237885462555\n",
      "Training Loss per 5000 steps: 0.3869588920334866\n",
      "Training Accuracy per 5000 steps: 89.50549450549451\n",
      "Training Loss per 5000 steps: 0.38619305158703865\n",
      "Training Accuracy per 5000 steps: 89.52850877192982\n",
      "Training Loss per 5000 steps: 0.38564320868472174\n",
      "Training Accuracy per 5000 steps: 89.55142231947484\n",
      "Training Loss per 5000 steps: 0.384857200533477\n",
      "Training Accuracy per 5000 steps: 89.57423580786026\n",
      "Training Loss per 5000 steps: 0.3848256913129411\n",
      "Training Accuracy per 5000 steps: 89.54248366013071\n",
      "Training Loss per 5000 steps: 0.3848387644585708\n",
      "Training Accuracy per 5000 steps: 89.51086956521739\n",
      "Training Loss per 5000 steps: 0.38502684671132786\n",
      "Training Accuracy per 5000 steps: 89.47939262472885\n",
      "Training Loss per 5000 steps: 0.38457433907597355\n",
      "Training Accuracy per 5000 steps: 89.5021645021645\n",
      "Training Loss per 5000 steps: 0.38454360351545763\n",
      "Training Accuracy per 5000 steps: 89.52483801295897\n",
      "Training Loss per 5000 steps: 0.38381379707074115\n",
      "Training Accuracy per 5000 steps: 89.54741379310344\n",
      "Training Loss per 5000 steps: 0.38309519224429645\n",
      "Training Accuracy per 5000 steps: 89.56989247311827\n",
      "Training Loss per 5000 steps: 0.38233303673197655\n",
      "Training Accuracy per 5000 steps: 89.5922746781116\n",
      "Training Loss per 5000 steps: 0.381606237133227\n",
      "Training Accuracy per 5000 steps: 89.61456102783725\n",
      "Training Loss per 5000 steps: 0.38134905668453145\n",
      "Training Accuracy per 5000 steps: 89.63675213675214\n",
      "Training Loss per 5000 steps: 0.38077965975681477\n",
      "Training Accuracy per 5000 steps: 89.6588486140725\n",
      "Training Loss per 5000 steps: 0.3802129989252129\n",
      "Training Accuracy per 5000 steps: 89.68085106382979\n",
      "Training Loss per 5000 steps: 0.3795107532956987\n",
      "Training Accuracy per 5000 steps: 89.70276008492569\n",
      "Training Loss per 5000 steps: 0.3787694743210923\n",
      "Training Accuracy per 5000 steps: 89.72457627118644\n",
      "Training Loss per 5000 steps: 0.378066724623075\n",
      "Training Accuracy per 5000 steps: 89.7463002114165\n",
      "Training Loss per 5000 steps: 0.38012782468181494\n",
      "Training Accuracy per 5000 steps: 89.71518987341773\n",
      "Training Loss per 5000 steps: 0.3793899370180933\n",
      "Training Accuracy per 5000 steps: 89.73684210526316\n",
      "Training Loss per 5000 steps: 0.3793381969405322\n",
      "Training Accuracy per 5000 steps: 89.70588235294117\n",
      "Training Loss per 5000 steps: 0.3785916848861821\n",
      "Training Accuracy per 5000 steps: 89.72746331236897\n",
      "Training Loss per 5000 steps: 0.37794373121315217\n",
      "Training Accuracy per 5000 steps: 89.7489539748954\n",
      "Training Loss per 5000 steps: 0.3772030761601532\n",
      "Training Accuracy per 5000 steps: 89.77035490605428\n",
      "Training Loss per 5000 steps: 0.3764609898750981\n",
      "Training Accuracy per 5000 steps: 89.79166666666667\n",
      "Training Loss per 5000 steps: 0.3757776985034006\n",
      "Training Accuracy per 5000 steps: 89.81288981288981\n",
      "Training Loss per 5000 steps: 0.37503831181288755\n",
      "Training Accuracy per 5000 steps: 89.83402489626556\n",
      "Training Loss per 5000 steps: 0.3743805158690513\n",
      "Training Accuracy per 5000 steps: 89.85507246376811\n",
      "Training Loss per 5000 steps: 0.37377000588554243\n",
      "Training Accuracy per 5000 steps: 89.87603305785125\n",
      "Training Loss per 5000 steps: 0.37317284957803404\n",
      "Training Accuracy per 5000 steps: 89.89690721649484\n",
      "Training Loss per 5000 steps: 0.3725698934676348\n",
      "Training Accuracy per 5000 steps: 89.91769547325103\n",
      "Training Loss per 5000 steps: 0.3719132804833888\n",
      "Training Accuracy per 5000 steps: 89.93839835728953\n",
      "Training Loss per 5000 steps: 0.37151953730671133\n",
      "Training Accuracy per 5000 steps: 89.95901639344262\n",
      "Training Loss per 5000 steps: 0.3712356589079391\n",
      "Training Accuracy per 5000 steps: 89.92842535787321\n",
      "Training Loss per 5000 steps: 0.37210379547002365\n",
      "Training Accuracy per 5000 steps: 89.89795918367346\n",
      "Training Loss per 5000 steps: 0.37179250423388666\n",
      "Training Accuracy per 5000 steps: 89.86761710794298\n",
      "Training Loss per 5000 steps: 0.37371852400341654\n",
      "Training Accuracy per 5000 steps: 89.83739837398375\n",
      "Training Loss per 5000 steps: 0.3731691568001772\n",
      "Training Accuracy per 5000 steps: 89.8580121703854\n",
      "Training Loss per 5000 steps: 0.372577170294668\n",
      "Training Accuracy per 5000 steps: 89.87854251012146\n",
      "Training Loss per 5000 steps: 0.3720903032807389\n",
      "Training Accuracy per 5000 steps: 89.8989898989899\n",
      "Training Loss per 5000 steps: 0.3717872642371203\n",
      "Training Accuracy per 5000 steps: 89.91935483870968\n",
      "Training Loss per 5000 steps: 0.37144015431104294\n",
      "Training Accuracy per 5000 steps: 89.93963782696177\n",
      "Training Loss per 5000 steps: 0.3707305335937195\n",
      "Training Accuracy per 5000 steps: 89.95983935742971\n",
      "Training Loss per 5000 steps: 0.37005418401308554\n",
      "Training Accuracy per 5000 steps: 89.97995991983969\n",
      "Training Loss per 5000 steps: 0.3693777842335403\n",
      "Training Accuracy per 5000 steps: 90.0\n",
      "Training Loss per 5000 steps: 0.3689526087026337\n",
      "Training Accuracy per 5000 steps: 90.01996007984032\n",
      "Training Loss per 5000 steps: 0.3693961011453632\n",
      "Training Accuracy per 5000 steps: 89.99003984063745\n",
      "Training Loss per 5000 steps: 0.3687075598843055\n",
      "Training Accuracy per 5000 steps: 90.00994035785288\n",
      "Training Loss per 5000 steps: 0.36849447684214703\n",
      "Training Accuracy per 5000 steps: 89.98015873015873\n",
      "Training Loss per 5000 steps: 0.36791240407688786\n",
      "Training Accuracy per 5000 steps: 90.0\n",
      "Training Loss per 5000 steps: 0.36738960148965416\n",
      "Training Accuracy per 5000 steps: 90.0197628458498\n",
      "Training Loss per 5000 steps: 0.3667282666533421\n",
      "Training Accuracy per 5000 steps: 90.03944773175543\n",
      "Training Loss per 5000 steps: 0.36632273382851926\n",
      "Training Accuracy per 5000 steps: 90.05905511811024\n",
      "Training Loss per 5000 steps: 0.36633486007953203\n",
      "Training Accuracy per 5000 steps: 90.02946954813359\n",
      "Training Loss per 5000 steps: 0.3658224912542923\n",
      "Training Accuracy per 5000 steps: 90.04901960784314\n",
      "Training Loss per 5000 steps: 0.36590040368693216\n",
      "Training Accuracy per 5000 steps: 90.01956947162427\n",
      "Training Loss per 5000 steps: 0.3653435985179385\n",
      "Training Accuracy per 5000 steps: 90.0390625\n",
      "Training Loss per 5000 steps: 0.3647133172516934\n",
      "Training Accuracy per 5000 steps: 90.05847953216374\n",
      "Training Loss per 5000 steps: 0.3651652037082711\n",
      "Training Accuracy per 5000 steps: 90.02918287937743\n",
      "Training Loss per 5000 steps: 0.36450461859914285\n",
      "Training Accuracy per 5000 steps: 90.04854368932038\n",
      "Training Loss per 5000 steps: 0.36406533603823577\n",
      "Training Accuracy per 5000 steps: 90.06782945736434\n",
      "Training Loss per 5000 steps: 0.36340602317273735\n",
      "Training Accuracy per 5000 steps: 90.08704061895551\n",
      "Training Loss per 5000 steps: 0.3650893750942122\n",
      "Training Accuracy per 5000 steps: 90.05791505791505\n",
      "Training Loss per 5000 steps: 0.3644868441012209\n",
      "Training Accuracy per 5000 steps: 90.07707129094412\n",
      "Training Loss per 5000 steps: 0.36382138395610336\n",
      "Training Accuracy per 5000 steps: 90.09615384615384\n",
      "Training Loss per 5000 steps: 0.36319668659307563\n",
      "Training Accuracy per 5000 steps: 90.1151631477927\n",
      "Training Loss per 5000 steps: 0.36254374399224576\n",
      "Training Accuracy per 5000 steps: 90.13409961685824\n",
      "Training Loss per 5000 steps: 0.36198916286586236\n",
      "Training Accuracy per 5000 steps: 90.1529636711281\n",
      "Training Loss per 5000 steps: 0.36403624885822183\n",
      "Training Accuracy per 5000 steps: 90.12404580152672\n",
      "Training Loss per 5000 steps: 0.3637477643894298\n",
      "Training Accuracy per 5000 steps: 90.14285714285714\n",
      "Training Loss per 5000 steps: 0.36316610234984315\n",
      "Training Accuracy per 5000 steps: 90.16159695817491\n",
      "Training Loss per 5000 steps: 0.36333855118185454\n",
      "Training Accuracy per 5000 steps: 90.13282732447819\n",
      "Training Loss per 5000 steps: 0.3628176624823192\n",
      "Training Accuracy per 5000 steps: 90.15151515151516\n",
      "Training Loss per 5000 steps: 0.362214097746125\n",
      "Training Accuracy per 5000 steps: 90.17013232514178\n",
      "Training Loss per 5000 steps: 0.36171367997479326\n",
      "Training Accuracy per 5000 steps: 90.18867924528301\n",
      "Training Loss per 5000 steps: 0.362011406461931\n",
      "Training Accuracy per 5000 steps: 90.16007532956685\n",
      "Training Loss per 5000 steps: 0.36137432161424504\n",
      "Training Accuracy per 5000 steps: 90.17857142857143\n",
      "Training Loss per 5000 steps: 0.36074977536907676\n",
      "Training Accuracy per 5000 steps: 90.1969981238274\n",
      "Training Loss per 5000 steps: 0.36011445941926723\n",
      "Training Accuracy per 5000 steps: 90.21535580524345\n",
      "Training Loss per 5000 steps: 0.35966739805989734\n",
      "Training Accuracy per 5000 steps: 90.23364485981308\n",
      "Training Loss per 5000 steps: 0.359040129147887\n",
      "Training Accuracy per 5000 steps: 90.2518656716418\n",
      "Training Loss per 5000 steps: 0.3598413138270545\n",
      "Training Accuracy per 5000 steps: 90.22346368715084\n",
      "Training Loss per 5000 steps: 0.3592151609391362\n",
      "Training Accuracy per 5000 steps: 90.24163568773234\n",
      "Training Loss per 5000 steps: 0.3586826165118609\n",
      "Training Accuracy per 5000 steps: 90.25974025974025\n",
      "Training Loss per 5000 steps: 0.35908951923872034\n",
      "Training Accuracy per 5000 steps: 90.23148148148148\n",
      "Training Loss per 5000 steps: 0.35846307085975043\n",
      "Training Accuracy per 5000 steps: 90.24953789279112\n",
      "Training Loss per 5000 steps: 0.3578369191459404\n",
      "Training Accuracy per 5000 steps: 90.26752767527675\n",
      "Training Loss per 5000 steps: 0.35728979194416743\n",
      "Training Accuracy per 5000 steps: 90.2854511970534\n",
      "Training Loss per 5000 steps: 0.35669539764057845\n",
      "Training Accuracy per 5000 steps: 90.3033088235294\n",
      "Training Loss per 5000 steps: 0.35617466804767967\n",
      "Training Accuracy per 5000 steps: 90.3211009174312\n",
      "Training Loss per 5000 steps: 0.35563251050700856\n",
      "Training Accuracy per 5000 steps: 90.33882783882784\n",
      "Training Loss per 5000 steps: 0.35544798608317674\n",
      "Training Accuracy per 5000 steps: 90.31078610603291\n",
      "Training Loss per 5000 steps: 0.3555421756009442\n",
      "Training Accuracy per 5000 steps: 90.28284671532846\n",
      "Training Loss per 5000 steps: 0.3566121223072211\n",
      "Training Accuracy per 5000 steps: 90.25500910746813\n",
      "Training Loss per 5000 steps: 0.3561149993200194\n",
      "Training Accuracy per 5000 steps: 90.27272727272727\n",
      "Training Loss per 5000 steps: 0.3555128365389989\n",
      "Training Accuracy per 5000 steps: 90.29038112522686\n",
      "Training Loss per 5000 steps: 0.3549092286232643\n",
      "Training Accuracy per 5000 steps: 90.30797101449275\n",
      "Training Loss per 5000 steps: 0.35450164518684524\n",
      "Training Accuracy per 5000 steps: 90.32549728752261\n",
      "Training Loss per 5000 steps: 0.35403602390418953\n",
      "Training Accuracy per 5000 steps: 90.34296028880867\n",
      "Training Loss per 5000 steps: 0.3534462239861757\n",
      "Training Accuracy per 5000 steps: 90.36036036036036\n",
      "Training Loss per 5000 steps: 0.35365632578743983\n",
      "Training Accuracy per 5000 steps: 90.33273381294964\n",
      "Training Loss per 5000 steps: 0.35314763381849823\n",
      "Training Accuracy per 5000 steps: 90.35008976660683\n",
      "Training Loss per 5000 steps: 0.3525576878132092\n",
      "Training Accuracy per 5000 steps: 90.3673835125448\n",
      "Training Loss per 5000 steps: 0.3525424178303194\n",
      "Training Accuracy per 5000 steps: 90.33989266547407\n",
      "Training Loss per 5000 steps: 0.35299606672820766\n",
      "Training Accuracy per 5000 steps: 90.3125\n",
      "Training Loss per 5000 steps: 0.3545993148795796\n",
      "Training Accuracy per 5000 steps: 90.28520499108734\n",
      "Training Loss per 5000 steps: 0.3543783776375906\n",
      "Training Accuracy per 5000 steps: 90.25800711743773\n",
      "Training Loss per 5000 steps: 0.35394075352427273\n",
      "Training Accuracy per 5000 steps: 90.2753108348135\n",
      "Training Loss per 5000 steps: 0.35338122987829096\n",
      "Training Accuracy per 5000 steps: 90.29255319148936\n",
      "Training Loss per 5000 steps: 0.35315262394075375\n",
      "Training Accuracy per 5000 steps: 90.30973451327434\n",
      "Training Loss per 5000 steps: 0.3525724788936836\n",
      "Training Accuracy per 5000 steps: 90.32685512367492\n",
      "Training Loss per 5000 steps: 0.3521054553186662\n",
      "Training Accuracy per 5000 steps: 90.34391534391534\n",
      "Training Loss per 5000 steps: 0.35217638376732946\n",
      "Training Accuracy per 5000 steps: 90.3169014084507\n",
      "Training Loss per 5000 steps: 0.35160948260178254\n",
      "Training Accuracy per 5000 steps: 90.33391915641477\n",
      "Training Loss per 5000 steps: 0.35145712170684545\n",
      "Training Accuracy per 5000 steps: 90.30701754385964\n",
      "Training Loss per 5000 steps: 0.35239697014178994\n",
      "Training Accuracy per 5000 steps: 90.28021015761821\n",
      "Training Loss per 5000 steps: 0.35190708005240745\n",
      "Training Accuracy per 5000 steps: 90.2972027972028\n",
      "Training Loss per 5000 steps: 0.3513573186624841\n",
      "Training Accuracy per 5000 steps: 90.31413612565446\n",
      "Training Loss per 5000 steps: 0.3508062722154922\n",
      "Training Accuracy per 5000 steps: 90.33101045296168\n",
      "Training Loss per 5000 steps: 0.35041712578871975\n",
      "Training Accuracy per 5000 steps: 90.34782608695652\n",
      "Training Loss per 5000 steps: 0.35206094947514227\n",
      "Training Accuracy per 5000 steps: 90.32118055555556\n",
      "Training Loss per 5000 steps: 0.3515325644516759\n",
      "Training Accuracy per 5000 steps: 90.33795493934142\n",
      "Training Loss per 5000 steps: 0.35130476764045576\n",
      "Training Accuracy per 5000 steps: 90.35467128027682\n",
      "Training Loss per 5000 steps: 0.3513816955703631\n",
      "Training Accuracy per 5000 steps: 90.32815198618307\n",
      "Training Loss per 5000 steps: 0.35112433706506574\n",
      "Training Accuracy per 5000 steps: 90.34482758620689\n",
      "Training Loss per 5000 steps: 0.3510054542319192\n",
      "Training Accuracy per 5000 steps: 90.36144578313252\n",
      "Training Loss per 5000 steps: 0.3507618861932218\n",
      "Training Accuracy per 5000 steps: 90.37800687285224\n",
      "Training Loss per 5000 steps: 0.3520971864628035\n",
      "Training Accuracy per 5000 steps: 90.3516295025729\n",
      "Training Loss per 5000 steps: 0.35167967186832466\n",
      "Training Accuracy per 5000 steps: 90.3681506849315\n",
      "Training Loss per 5000 steps: 0.35130646789812636\n",
      "Training Accuracy per 5000 steps: 90.38461538461539\n",
      "Training Loss per 5000 steps: 0.3508638038710014\n",
      "Training Accuracy per 5000 steps: 90.40102389078498\n",
      "Training Loss per 5000 steps: 0.350551808655922\n",
      "Training Accuracy per 5000 steps: 90.41737649063032\n",
      "Training Loss per 5000 steps: 0.3501839673425369\n",
      "Training Accuracy per 5000 steps: 90.43367346938776\n",
      "Training Loss per 5000 steps: 0.34966286336955676\n",
      "Training Accuracy per 5000 steps: 90.44991511035654\n",
      "Training Loss per 5000 steps: 0.34941473904428844\n",
      "Training Accuracy per 5000 steps: 90.46610169491525\n",
      "Training Loss per 5000 steps: 0.34896396900418447\n",
      "Training Accuracy per 5000 steps: 90.48223350253807\n",
      "Training Loss per 5000 steps: 0.3488631070830633\n",
      "Training Accuracy per 5000 steps: 90.45608108108108\n",
      "Training Loss per 5000 steps: 0.3493517794619624\n",
      "Training Accuracy per 5000 steps: 90.43001686340641\n",
      "Training Loss per 5000 steps: 0.3489713658976936\n",
      "Training Accuracy per 5000 steps: 90.44612794612794\n",
      "Training Loss per 5000 steps: 0.3485986670463526\n",
      "Training Accuracy per 5000 steps: 90.46218487394958\n",
      "Training Loss per 5000 steps: 0.34805980654145846\n",
      "Training Accuracy per 5000 steps: 90.47818791946308\n",
      "Training Loss per 5000 steps: 0.3475066146296972\n",
      "Training Accuracy per 5000 steps: 90.49413735343384\n",
      "Training Loss per 5000 steps: 0.3471656817893021\n",
      "Training Accuracy per 5000 steps: 90.51003344481606\n",
      "Training Loss per 5000 steps: 0.34691328643036007\n",
      "Training Accuracy per 5000 steps: 90.52587646076795\n",
      "Training Loss per 5000 steps: 0.3465643170165519\n",
      "Training Accuracy per 5000 steps: 90.54166666666667\n",
      "Training Loss per 5000 steps: 0.34666128227868415\n",
      "Training Accuracy per 5000 steps: 90.51580698835275\n",
      "Training Loss per 5000 steps: 0.34629365199700346\n",
      "Training Accuracy per 5000 steps: 90.53156146179403\n",
      "Training Loss per 5000 steps: 0.3457917686409717\n",
      "Training Accuracy per 5000 steps: 90.54726368159204\n",
      "Training Loss per 5000 steps: 0.3478738706719303\n",
      "Training Accuracy per 5000 steps: 90.48013245033113\n",
      "Training Loss per 5000 steps: 0.3474079175736786\n",
      "Training Accuracy per 5000 steps: 90.49586776859505\n",
      "Training Loss per 5000 steps: 0.34767429518940624\n",
      "Training Accuracy per 5000 steps: 90.47029702970298\n",
      "Training Loss per 5000 steps: 0.34940186769531234\n",
      "Training Accuracy per 5000 steps: 90.44481054365733\n",
      "Training Loss per 5000 steps: 0.3488930418082562\n",
      "Training Accuracy per 5000 steps: 90.46052631578948\n",
      "Training Loss per 5000 steps: 0.3484469103230827\n",
      "Training Accuracy per 5000 steps: 90.47619047619048\n",
      "Training Loss per 5000 steps: 0.34916025538180695\n",
      "Training Accuracy per 5000 steps: 90.40983606557377\n",
      "Training Loss per 5000 steps: 0.34956086014813564\n",
      "Training Accuracy per 5000 steps: 90.38461538461539\n",
      "Training Loss per 5000 steps: 0.34905830884559286\n",
      "Training Accuracy per 5000 steps: 90.40032679738562\n",
      "Training Loss per 5000 steps: 0.34854967304509965\n",
      "Training Accuracy per 5000 steps: 90.41598694942904\n",
      "Training Loss per 5000 steps: 0.348016022112985\n",
      "Training Accuracy per 5000 steps: 90.43159609120521\n",
      "Training Loss per 5000 steps: 0.34756275881233256\n",
      "Training Accuracy per 5000 steps: 90.44715447154472\n",
      "Training Loss per 5000 steps: 0.34901055665202924\n",
      "Training Accuracy per 5000 steps: 90.42207792207792\n",
      "Training Loss per 5000 steps: 0.350580914389807\n",
      "Training Accuracy per 5000 steps: 90.3970826580227\n",
      "Training Loss per 5000 steps: 0.35007225295896477\n",
      "Training Accuracy per 5000 steps: 90.4126213592233\n",
      "Training Loss per 5000 steps: 0.3498358999067535\n",
      "Training Accuracy per 5000 steps: 90.4281098546042\n",
      "Training Loss per 5000 steps: 0.34931665121487554\n",
      "Training Accuracy per 5000 steps: 90.44354838709677\n",
      "Training Loss per 5000 steps: 0.3488019204604021\n",
      "Training Accuracy per 5000 steps: 90.45893719806763\n",
      "Training Loss per 5000 steps: 0.3483405892865978\n",
      "Training Accuracy per 5000 steps: 90.47427652733118\n",
      "Training Loss per 5000 steps: 0.34782498250648164\n",
      "Training Accuracy per 5000 steps: 90.48956661316213\n",
      "Training Loss per 5000 steps: 0.34737438985254043\n",
      "Training Accuracy per 5000 steps: 90.5048076923077\n",
      "Training Loss per 5000 steps: 0.346985955068469\n",
      "Training Accuracy per 5000 steps: 90.52\n",
      "Training Loss per 5000 steps: 0.3465337494043068\n",
      "Training Accuracy per 5000 steps: 90.53514376996804\n",
      "Training Loss per 5000 steps: 0.3460174816344201\n",
      "Training Accuracy per 5000 steps: 90.55023923444976\n",
      "Training Loss per 5000 steps: 0.345507089069386\n",
      "Training Accuracy per 5000 steps: 90.56528662420382\n",
      "Training Loss per 5000 steps: 0.34503853359275954\n",
      "Training Accuracy per 5000 steps: 90.58028616852147\n",
      "Training Loss per 5000 steps: 0.34453291513084894\n",
      "Training Accuracy per 5000 steps: 90.5952380952381\n",
      "Training Loss per 5000 steps: 0.34573539216278\n",
      "Training Accuracy per 5000 steps: 90.57052297939778\n",
      "Training Loss per 5000 steps: 0.34526101570823997\n",
      "Training Accuracy per 5000 steps: 90.58544303797468\n",
      "Training Loss per 5000 steps: 0.34478101545965406\n",
      "Training Accuracy per 5000 steps: 90.60031595576619\n",
      "Training Loss per 5000 steps: 0.3442874871376514\n",
      "Training Accuracy per 5000 steps: 90.61514195583597\n",
      "Training Loss per 5000 steps: 0.3442584364403655\n",
      "Training Accuracy per 5000 steps: 90.59055118110236\n",
      "Training Loss per 5000 steps: 0.34376753700030205\n",
      "Training Accuracy per 5000 steps: 90.60534591194968\n",
      "Training Loss per 5000 steps: 0.3432615207745058\n",
      "Training Accuracy per 5000 steps: 90.62009419152277\n",
      "Training Loss per 5000 steps: 0.34418900728774576\n",
      "Training Accuracy per 5000 steps: 90.59561128526646\n",
      "Training Loss per 5000 steps: 0.34368499285644954\n",
      "Training Accuracy per 5000 steps: 90.61032863849765\n",
      "Training Loss per 5000 steps: 0.34319321912480516\n",
      "Training Accuracy per 5000 steps: 90.625\n",
      "Training Loss per 5000 steps: 0.3427790288759654\n",
      "Training Accuracy per 5000 steps: 90.6396255850234\n",
      "Training Loss per 5000 steps: 0.3423010576052179\n",
      "Training Accuracy per 5000 steps: 90.65420560747664\n",
      "Training Loss per 5000 steps: 0.34182990640170097\n",
      "Training Accuracy per 5000 steps: 90.6687402799378\n",
      "Training Loss per 5000 steps: 0.3413539570322411\n",
      "Training Accuracy per 5000 steps: 90.6832298136646\n",
      "Training Loss per 5000 steps: 0.34101590528622155\n",
      "Training Accuracy per 5000 steps: 90.69767441860465\n",
      "Training Loss per 5000 steps: 0.34058776882479985\n",
      "Training Accuracy per 5000 steps: 90.71207430340557\n",
      "Training Loss per 5000 steps: 0.34041699441797585\n",
      "Training Accuracy per 5000 steps: 90.72642967542504\n",
      "Training Loss per 5000 steps: 0.34075183919803415\n",
      "Training Accuracy per 5000 steps: 90.70216049382717\n",
      "Training Loss per 5000 steps: 0.340356060836137\n",
      "Training Accuracy per 5000 steps: 90.71648690292758\n",
      "Training Loss per 5000 steps: 0.34008288147930915\n",
      "Training Accuracy per 5000 steps: 90.73076923076923\n",
      "Training Loss per 5000 steps: 0.3396524207783826\n",
      "Training Accuracy per 5000 steps: 90.74500768049155\n",
      "Training Loss per 5000 steps: 0.3392347227267784\n",
      "Training Accuracy per 5000 steps: 90.75920245398773\n",
      "Training Loss per 5000 steps: 0.3387604184095016\n",
      "Training Accuracy per 5000 steps: 90.77335375191424\n",
      "Training Loss per 5000 steps: 0.33847150882130733\n",
      "Training Accuracy per 5000 steps: 90.7874617737003\n",
      "Training Loss per 5000 steps: 0.33834014417788455\n",
      "Training Accuracy per 5000 steps: 90.76335877862596\n",
      "Training Loss per 5000 steps: 0.33786867713419405\n",
      "Training Accuracy per 5000 steps: 90.77743902439025\n",
      "Training Loss per 5000 steps: 0.33745447343252205\n",
      "Training Accuracy per 5000 steps: 90.79147640791476\n",
      "Training Loss per 5000 steps: 0.33711059282797085\n",
      "Training Accuracy per 5000 steps: 90.80547112462006\n",
      "Training Loss per 5000 steps: 0.3385293274349074\n",
      "Training Accuracy per 5000 steps: 90.7814871016692\n",
      "Training Loss per 5000 steps: 0.33833856817447777\n",
      "Training Accuracy per 5000 steps: 90.79545454545455\n",
      "Training Loss per 5000 steps: 0.33806437373431836\n",
      "Training Accuracy per 5000 steps: 90.80937972768533\n",
      "Training Loss per 5000 steps: 0.33766889107326364\n",
      "Training Accuracy per 5000 steps: 90.82326283987915\n",
      "Training Loss per 5000 steps: 0.33722884419794175\n",
      "Training Accuracy per 5000 steps: 90.83710407239819\n",
      "Training Loss per 5000 steps: 0.33688076361677194\n",
      "Training Accuracy per 5000 steps: 90.85090361445783\n",
      "Training Loss per 5000 steps: 0.3370228218909045\n",
      "Training Accuracy per 5000 steps: 90.82706766917293\n",
      "Training Loss per 5000 steps: 0.3365862202794255\n",
      "Training Accuracy per 5000 steps: 90.84084084084084\n",
      "Training Loss per 5000 steps: 0.3361091142774433\n",
      "Training Accuracy per 5000 steps: 90.85457271364318\n",
      "Training Loss per 5000 steps: 0.335702639647542\n",
      "Training Accuracy per 5000 steps: 90.86826347305389\n",
      "Training Loss per 5000 steps: 0.335282203795664\n",
      "Training Accuracy per 5000 steps: 90.88191330343797\n",
      "Training Loss per 5000 steps: 0.3348669770827044\n",
      "Training Accuracy per 5000 steps: 90.8955223880597\n",
      "Training Loss per 5000 steps: 0.3347828156702032\n",
      "Training Accuracy per 5000 steps: 90.87183308494784\n",
      "Training Loss per 5000 steps: 0.33441156933882404\n",
      "Training Accuracy per 5000 steps: 90.88541666666667\n",
      "Training Loss per 5000 steps: 0.33587184406673043\n",
      "Training Accuracy per 5000 steps: 90.86181277860327\n",
      "Training Loss per 5000 steps: 0.3354162725534301\n",
      "Training Accuracy per 5000 steps: 90.8753709198813\n",
      "Training Loss per 5000 steps: 0.33496275730155134\n",
      "Training Accuracy per 5000 steps: 90.88888888888889\n",
      "Training Loss per 5000 steps: 0.3347439637694221\n",
      "Training Accuracy per 5000 steps: 90.90236686390533\n",
      "Training Loss per 5000 steps: 0.3342853676874405\n",
      "Training Accuracy per 5000 steps: 90.91580502215658\n",
      "Training Loss per 5000 steps: 0.33385237361851167\n",
      "Training Accuracy per 5000 steps: 90.929203539823\n",
      "Training Loss per 5000 steps: 0.3334603413709046\n",
      "Training Accuracy per 5000 steps: 90.94256259204712\n",
      "Training Loss per 5000 steps: 0.3330022100715295\n",
      "Training Accuracy per 5000 steps: 90.95588235294117\n",
      "Training Loss per 5000 steps: 0.33266326765667936\n",
      "Training Accuracy per 5000 steps: 90.96916299559471\n",
      "Training Loss per 5000 steps: 0.33252610557076256\n",
      "Training Accuracy per 5000 steps: 90.98240469208211\n",
      "Training Loss per 5000 steps: 0.3321014527002374\n",
      "Training Accuracy per 5000 steps: 90.99560761346999\n",
      "Training Loss per 5000 steps: 0.3316551902963186\n",
      "Training Accuracy per 5000 steps: 91.00877192982456\n",
      "Training Loss per 5000 steps: 0.3314026932510799\n",
      "Training Accuracy per 5000 steps: 91.02189781021897\n",
      "Training Loss per 5000 steps: 0.3310358301646812\n",
      "Training Accuracy per 5000 steps: 91.03498542274052\n",
      "Training Loss per 5000 steps: 0.330574184877112\n",
      "Training Accuracy per 5000 steps: 91.04803493449782\n",
      "Training Loss per 5000 steps: 0.3301294662185598\n",
      "Training Accuracy per 5000 steps: 91.06104651162791\n",
      "Training Loss per 5000 steps: 0.32968978577189373\n",
      "Training Accuracy per 5000 steps: 91.07402031930334\n",
      "Training Loss per 5000 steps: 0.33055359479174884\n",
      "Training Accuracy per 5000 steps: 91.05072463768116\n",
      "Training Loss per 5000 steps: 0.33014937799003424\n",
      "Training Accuracy per 5000 steps: 91.06367583212734\n",
      "Training Loss per 5000 steps: 0.329758847448598\n",
      "Training Accuracy per 5000 steps: 91.07658959537572\n",
      "Training Loss per 5000 steps: 0.3310707555358092\n",
      "Training Accuracy per 5000 steps: 91.05339105339105\n",
      "Training Loss per 5000 steps: 0.33098774884695925\n",
      "Training Accuracy per 5000 steps: 91.03025936599424\n",
      "Training Loss per 5000 steps: 0.3309836134890751\n",
      "Training Accuracy per 5000 steps: 91.00719424460432\n",
      "Training Loss per 5000 steps: 0.33144693712880513\n",
      "Training Accuracy per 5000 steps: 90.98419540229885\n",
      "Training Loss per 5000 steps: 0.33101476928943413\n",
      "Training Accuracy per 5000 steps: 90.99713055954089\n",
      "Training Loss per 5000 steps: 0.3305762207367722\n",
      "Training Accuracy per 5000 steps: 91.01002865329512\n",
      "Training Loss per 5000 steps: 0.33021466172991776\n",
      "Training Accuracy per 5000 steps: 91.02288984263234\n",
      "Training Loss per 5000 steps: 0.3298296214227698\n",
      "Training Accuracy per 5000 steps: 91.03571428571429\n",
      "Training Loss per 5000 steps: 0.32941620069530697\n",
      "Training Accuracy per 5000 steps: 91.04850213980029\n",
      "Training Loss per 5000 steps: 0.32978142214875317\n",
      "Training Accuracy per 5000 steps: 91.02564102564102\n",
      "Training Loss per 5000 steps: 0.3294280562536582\n",
      "Training Accuracy per 5000 steps: 91.03840682788051\n",
      "Training Loss per 5000 steps: 0.32958541504260874\n",
      "Training Accuracy per 5000 steps: 91.015625\n",
      "Training Loss per 5000 steps: 0.3294441471140541\n",
      "Training Accuracy per 5000 steps: 91.02836879432624\n",
      "Training Loss per 5000 steps: 0.3301324056100005\n",
      "Training Accuracy per 5000 steps: 91.0056657223796\n",
      "Training Loss per 5000 steps: 0.3301177504821516\n",
      "Training Accuracy per 5000 steps: 90.98302687411598\n",
      "Training Loss per 5000 steps: 0.3297548941428652\n",
      "Training Accuracy per 5000 steps: 90.9957627118644\n",
      "Training Loss per 5000 steps: 0.32932902889507487\n",
      "Training Accuracy per 5000 steps: 91.00846262341325\n",
      "Training Loss per 5000 steps: 0.32890932396395317\n",
      "Training Accuracy per 5000 steps: 91.02112676056338\n",
      "Training Loss per 5000 steps: 0.32881518846453706\n",
      "Training Accuracy per 5000 steps: 91.03375527426161\n",
      "Training Loss per 5000 steps: 0.32848447010972737\n",
      "Training Accuracy per 5000 steps: 91.04634831460675\n",
      "Training Loss per 5000 steps: 0.3281044872467201\n",
      "Training Accuracy per 5000 steps: 91.05890603085554\n",
      "Training Loss per 5000 steps: 0.3279944160543591\n",
      "Training Accuracy per 5000 steps: 91.07142857142857\n",
      "Training Loss per 5000 steps: 0.3278137738067474\n",
      "Training Accuracy per 5000 steps: 91.08391608391608\n",
      "Training Loss per 5000 steps: 0.32740579648645773\n",
      "Training Accuracy per 5000 steps: 91.0963687150838\n",
      "Training Loss per 5000 steps: 0.32703188053531\n",
      "Training Accuracy per 5000 steps: 91.10878661087867\n",
      "Training Loss per 5000 steps: 0.3266804111319851\n",
      "Training Accuracy per 5000 steps: 91.12116991643454\n",
      "Training Loss per 5000 steps: 0.3262546045488065\n",
      "Training Accuracy per 5000 steps: 91.13351877607789\n",
      "Training Loss per 5000 steps: 0.3258316082918706\n",
      "Training Accuracy per 5000 steps: 91.14583333333333\n",
      "Training Loss per 5000 steps: 0.3254068174106217\n",
      "Training Accuracy per 5000 steps: 91.15811373092926\n",
      "Training Loss per 5000 steps: 0.32499159687944273\n",
      "Training Accuracy per 5000 steps: 91.17036011080333\n",
      "Training Loss per 5000 steps: 0.32462175657398185\n",
      "Training Accuracy per 5000 steps: 91.18257261410788\n",
      "Training Loss per 5000 steps: 0.3251122498372827\n",
      "Training Accuracy per 5000 steps: 91.16022099447514\n",
      "Training Loss per 5000 steps: 0.3247775831317593\n",
      "Training Accuracy per 5000 steps: 91.17241379310344\n",
      "Training Loss per 5000 steps: 0.3244955404661596\n",
      "Training Accuracy per 5000 steps: 91.18457300275482\n",
      "Training Loss per 5000 steps: 0.3240749652560242\n",
      "Training Accuracy per 5000 steps: 91.19669876203577\n",
      "Training Loss per 5000 steps: 0.3239089908669345\n",
      "Training Accuracy per 5000 steps: 91.17445054945055\n",
      "Training Loss per 5000 steps: 0.3235123702791227\n",
      "Training Accuracy per 5000 steps: 91.18655692729767\n",
      "Training Loss per 5000 steps: 0.32311194212852073\n",
      "Training Accuracy per 5000 steps: 91.1986301369863\n",
      "Training Loss per 5000 steps: 0.3242597419363427\n",
      "Training Accuracy per 5000 steps: 91.14227086183311\n",
      "Training Loss per 5000 steps: 0.3238942598748464\n",
      "Training Accuracy per 5000 steps: 91.15437158469945\n",
      "Training Loss per 5000 steps: 0.32371735563031606\n",
      "Training Accuracy per 5000 steps: 91.16643929058662\n",
      "Training Loss per 5000 steps: 0.3233047403555049\n",
      "Training Accuracy per 5000 steps: 91.17847411444141\n",
      "Training Loss per 5000 steps: 0.32574790462116604\n",
      "Training Accuracy per 5000 steps: 91.12244897959184\n",
      "Training Loss per 5000 steps: 0.32670218996961764\n",
      "Training Accuracy per 5000 steps: 91.06657608695652\n",
      "Training Loss per 5000 steps: 0.32630836342308506\n",
      "Training Accuracy per 5000 steps: 91.078697421981\n",
      "Training Loss per 5000 steps: 0.325899628148765\n",
      "Training Accuracy per 5000 steps: 91.09078590785907\n",
      "Training Loss per 5000 steps: 0.3263764191350833\n",
      "Training Accuracy per 5000 steps: 91.06901217861976\n",
      "Training Loss per 5000 steps: 0.32698872913858174\n",
      "Training Accuracy per 5000 steps: 91.04729729729729\n",
      "Training Loss per 5000 steps: 0.32668894256732844\n",
      "Training Accuracy per 5000 steps: 91.05937921727396\n",
      "Training Loss per 5000 steps: 0.32630287834149446\n",
      "Training Accuracy per 5000 steps: 91.07142857142857\n",
      "Training Loss per 5000 steps: 0.3259045241806721\n",
      "Training Accuracy per 5000 steps: 91.08344549125168\n",
      "Training Loss per 5000 steps: 0.3255036517875069\n",
      "Training Accuracy per 5000 steps: 91.09543010752688\n",
      "Training Loss per 5000 steps: 0.3255528522155329\n",
      "Training Accuracy per 5000 steps: 91.0738255033557\n",
      "Training Loss per 5000 steps: 0.32517596112873814\n",
      "Training Accuracy per 5000 steps: 91.08579088471849\n",
      "Training Loss per 5000 steps: 0.3252901358335432\n",
      "Training Accuracy per 5000 steps: 91.06425702811245\n",
      "Training Loss per 5000 steps: 0.3250351587875701\n",
      "Training Accuracy per 5000 steps: 91.07620320855615\n",
      "Training Loss per 5000 steps: 0.3246866039103656\n",
      "Training Accuracy per 5000 steps: 91.08811748998664\n",
      "Training Loss per 5000 steps: 0.32430871365343533\n",
      "Training Accuracy per 5000 steps: 91.1\n",
      "Training Loss per 5000 steps: 0.32437029161357805\n",
      "Training Accuracy per 5000 steps: 91.07856191744341\n",
      "Training Loss per 5000 steps: 0.32440221842571576\n",
      "Training Accuracy per 5000 steps: 91.05718085106383\n",
      "Training Loss per 5000 steps: 0.3247000853881082\n",
      "Training Accuracy per 5000 steps: 91.03585657370517\n",
      "Training Loss per 5000 steps: 0.32443151974665196\n",
      "Training Accuracy per 5000 steps: 91.04774535809018\n",
      "Training Loss per 5000 steps: 0.3240663888426313\n",
      "Training Accuracy per 5000 steps: 91.05960264900662\n",
      "Training Loss per 5000 steps: 0.32367977065714226\n",
      "Training Accuracy per 5000 steps: 91.07142857142857\n",
      "Training Loss per 5000 steps: 0.3233295391292817\n",
      "Training Accuracy per 5000 steps: 91.08322324966974\n",
      "Training Loss per 5000 steps: 0.3229729603118408\n",
      "Training Accuracy per 5000 steps: 91.09498680738787\n",
      "Training Loss per 5000 steps: 0.3227579771468664\n",
      "Training Accuracy per 5000 steps: 91.10671936758894\n",
      "Training Loss per 5000 steps: 0.32256323784761326\n",
      "Training Accuracy per 5000 steps: 91.11842105263158\n",
      "Training Loss per 5000 steps: 0.3221922702634519\n",
      "Training Accuracy per 5000 steps: 91.13009198423127\n",
      "Training Loss per 5000 steps: 0.32196879648821053\n",
      "Training Accuracy per 5000 steps: 91.14173228346456\n",
      "Training Loss per 5000 steps: 0.32159836372900097\n",
      "Training Accuracy per 5000 steps: 91.15334207077326\n",
      "Training Loss per 5000 steps: 0.32156593110095416\n",
      "Training Accuracy per 5000 steps: 91.13219895287958\n",
      "Training Loss per 5000 steps: 0.3221394523625183\n",
      "Training Accuracy per 5000 steps: 91.11111111111111\n",
      "Training Loss per 5000 steps: 0.32199674569843606\n",
      "Training Accuracy per 5000 steps: 91.12271540469973\n",
      "Training Loss per 5000 steps: 0.3225383493294053\n",
      "Training Accuracy per 5000 steps: 91.10169491525424\n",
      "Training Loss per 5000 steps: 0.3222879449155395\n",
      "Training Accuracy per 5000 steps: 91.11328125\n",
      "Training Loss per 5000 steps: 0.32201279882312556\n",
      "Training Accuracy per 5000 steps: 91.12483745123536\n",
      "Training Loss per 5000 steps: 0.32177259115405477\n",
      "Training Accuracy per 5000 steps: 91.13636363636364\n",
      "Training Loss per 5000 steps: 0.32153308219922616\n",
      "Training Accuracy per 5000 steps: 91.147859922179\n",
      "Training Loss per 5000 steps: 0.32123908762810416\n",
      "Training Accuracy per 5000 steps: 91.15932642487047\n",
      "Training Loss per 5000 steps: 0.32085809160390194\n",
      "Training Accuracy per 5000 steps: 91.17076326002588\n",
      "Training Loss per 5000 steps: 0.32142920001441233\n",
      "Training Accuracy per 5000 steps: 91.14987080103359\n",
      "Training Loss per 5000 steps: 0.3210365888945037\n",
      "Training Accuracy per 5000 steps: 91.16129032258064\n",
      "Training Loss per 5000 steps: 0.32102478639169874\n",
      "Training Accuracy per 5000 steps: 91.14046391752578\n",
      "Training Loss per 5000 steps: 0.32070809121254445\n",
      "Training Accuracy per 5000 steps: 91.15186615186616\n",
      "Training Loss per 5000 steps: 0.3203289709341231\n",
      "Training Accuracy per 5000 steps: 91.16323907455013\n",
      "Training Loss per 5000 steps: 0.31997414039799393\n",
      "Training Accuracy per 5000 steps: 91.17458279845957\n",
      "Training Loss per 5000 steps: 0.3201659178253836\n",
      "Training Accuracy per 5000 steps: 91.15384615384616\n",
      "Training Loss per 5000 steps: 0.31978682971770533\n",
      "Training Accuracy per 5000 steps: 91.1651728553137\n",
      "Training Loss per 5000 steps: 0.3194057003409147\n",
      "Training Accuracy per 5000 steps: 91.17647058823529\n",
      "Training Loss per 5000 steps: 0.3190261292619819\n",
      "Training Accuracy per 5000 steps: 91.18773946360153\n",
      "Training Loss per 5000 steps: 0.31868814358283404\n",
      "Training Accuracy per 5000 steps: 91.19897959183673\n",
      "Training Loss per 5000 steps: 0.3183644677387776\n",
      "Training Accuracy per 5000 steps: 91.21019108280255\n",
      "Training Loss per 5000 steps: 0.31802859432810476\n",
      "Training Accuracy per 5000 steps: 91.22137404580153\n",
      "Training Loss per 5000 steps: 0.31765462497067004\n",
      "Training Accuracy per 5000 steps: 91.23252858958068\n",
      "Training Loss per 5000 steps: 0.31732432697048685\n",
      "Training Accuracy per 5000 steps: 91.24365482233503\n",
      "Training Loss per 5000 steps: 0.31694925734973944\n",
      "Training Accuracy per 5000 steps: 91.25475285171103\n",
      "Training Loss per 5000 steps: 0.3165729629264885\n",
      "Training Accuracy per 5000 steps: 91.26582278481013\n",
      "Training Loss per 5000 steps: 0.3162062397019469\n",
      "Training Accuracy per 5000 steps: 91.27686472819217\n",
      "Training Loss per 5000 steps: 0.3160372813328668\n",
      "Training Accuracy per 5000 steps: 91.28787878787878\n",
      "Training Loss per 5000 steps: 0.3157012087233073\n",
      "Training Accuracy per 5000 steps: 91.29886506935688\n",
      "Training Loss per 5000 steps: 0.3153710705040478\n",
      "Training Accuracy per 5000 steps: 91.30982367758186\n",
      "Training Loss per 5000 steps: 0.3150603710351594\n",
      "Training Accuracy per 5000 steps: 91.32075471698113\n",
      "Training Loss per 5000 steps: 0.31472393970154977\n",
      "Training Accuracy per 5000 steps: 91.33165829145729\n",
      "Training Loss per 5000 steps: 0.3143987142804594\n",
      "Training Accuracy per 5000 steps: 91.34253450439147\n",
      "Training Loss per 5000 steps: 0.31402765693292395\n",
      "Training Accuracy per 5000 steps: 91.35338345864662\n",
      "Training Loss per 5000 steps: 0.31366178996101\n",
      "Training Accuracy per 5000 steps: 91.36420525657071\n",
      "Training Loss per 5000 steps: 0.3137909505364951\n",
      "Training Accuracy per 5000 steps: 91.34375\n",
      "Training Loss per 5000 steps: 0.31348347749802913\n",
      "Training Accuracy per 5000 steps: 91.35455680399501\n",
      "Training Loss per 5000 steps: 0.3133708739739162\n",
      "Training Accuracy per 5000 steps: 91.33416458852868\n",
      "Training Loss per 5000 steps: 0.3145156646203041\n",
      "Training Accuracy per 5000 steps: 91.31382316313824\n",
      "Training Loss per 5000 steps: 0.3142252777696156\n",
      "Training Accuracy per 5000 steps: 91.32462686567165\n",
      "Training Loss per 5000 steps: 0.31389294369782533\n",
      "Training Accuracy per 5000 steps: 91.33540372670808\n",
      "Training Loss per 5000 steps: 0.31354528786059904\n",
      "Training Accuracy per 5000 steps: 91.34615384615384\n",
      "Training Loss per 5000 steps: 0.3131789996252941\n",
      "Training Accuracy per 5000 steps: 91.35687732342008\n",
      "Training Loss per 5000 steps: 0.31306788714551875\n",
      "Training Accuracy per 5000 steps: 91.36757425742574\n",
      "Training Loss per 5000 steps: 0.3128202789289271\n",
      "Training Accuracy per 5000 steps: 91.37824474660074\n",
      "Training Loss per 5000 steps: 0.31245340396431676\n",
      "Training Accuracy per 5000 steps: 91.38888888888889\n",
      "Training Loss per 5000 steps: 0.3121616981163364\n",
      "Training Accuracy per 5000 steps: 91.39950678175093\n",
      "Training Loss per 5000 steps: 0.3133424932717488\n",
      "Training Accuracy per 5000 steps: 91.37931034482759\n",
      "Training Loss per 5000 steps: 0.3130489875875221\n",
      "Training Accuracy per 5000 steps: 91.38991389913899\n",
      "Training Loss per 5000 steps: 0.3138562322214374\n",
      "Training Accuracy per 5000 steps: 91.36977886977887\n",
      "Training Loss per 5000 steps: 0.31358540939064294\n",
      "Training Accuracy per 5000 steps: 91.38036809815951\n",
      "Training Loss per 5000 steps: 0.3132970512299068\n",
      "Training Accuracy per 5000 steps: 91.39093137254902\n",
      "Training Loss per 5000 steps: 0.3129728620010115\n",
      "Training Accuracy per 5000 steps: 91.40146878824969\n",
      "Training Loss per 5000 steps: 0.3126061678144507\n",
      "Training Accuracy per 5000 steps: 91.41198044009779\n",
      "Training Loss per 5000 steps: 0.31224821490304994\n",
      "Training Accuracy per 5000 steps: 91.42246642246643\n",
      "Training Loss per 5000 steps: 0.31202627682672224\n",
      "Training Accuracy per 5000 steps: 91.4329268292683\n",
      "Training Loss per 5000 steps: 0.31175575049278687\n",
      "Training Accuracy per 5000 steps: 91.44336175395858\n",
      "Training Loss per 5000 steps: 0.31150927720591426\n",
      "Training Accuracy per 5000 steps: 91.45377128953771\n",
      "Training Loss per 5000 steps: 0.3112328934697106\n",
      "Training Accuracy per 5000 steps: 91.46415552855407\n",
      "Training Loss per 5000 steps: 0.31211310726911445\n",
      "Training Accuracy per 5000 steps: 91.44417475728156\n",
      "Training Loss per 5000 steps: 0.3121607314513037\n",
      "Training Accuracy per 5000 steps: 91.45454545454545\n",
      "Training Loss per 5000 steps: 0.3121096369197037\n",
      "Training Accuracy per 5000 steps: 91.43462469733656\n",
      "Training Loss per 5000 steps: 0.3131374991414427\n",
      "Training Accuracy per 5000 steps: 91.41475211608223\n",
      "Training Loss per 5000 steps: 0.31281631203831717\n",
      "Training Accuracy per 5000 steps: 91.42512077294685\n",
      "Training Loss per 5000 steps: 0.312581224509698\n",
      "Training Accuracy per 5000 steps: 91.43546441495778\n",
      "Training Loss per 5000 steps: 0.3123230090218944\n",
      "Training Accuracy per 5000 steps: 91.44578313253012\n",
      "Training Loss per 5000 steps: 0.31197636879904356\n",
      "Training Accuracy per 5000 steps: 91.4560770156438\n",
      "Training Loss per 5000 steps: 0.3116379850107478\n",
      "Training Accuracy per 5000 steps: 91.46634615384616\n",
      "Training Loss per 5000 steps: 0.31128946174437006\n",
      "Training Accuracy per 5000 steps: 91.4765906362545\n",
      "Training Loss per 5000 steps: 0.3111003811216558\n",
      "Training Accuracy per 5000 steps: 91.48681055155875\n",
      "Training Loss per 5000 steps: 0.3107618616960809\n",
      "Training Accuracy per 5000 steps: 91.49700598802396\n",
      "Training Loss per 5000 steps: 0.3105138584884534\n",
      "Training Accuracy per 5000 steps: 91.50717703349282\n",
      "Training Loss per 5000 steps: 0.3101837392306687\n",
      "Training Accuracy per 5000 steps: 91.51732377538829\n",
      "Training Loss per 5000 steps: 0.3099265198872329\n",
      "Training Accuracy per 5000 steps: 91.527446300716\n",
      "Training Loss per 5000 steps: 0.3096130664634637\n",
      "Training Accuracy per 5000 steps: 91.53754469606675\n",
      "Training Loss per 5000 steps: 0.3092746204197673\n",
      "Training Accuracy per 5000 steps: 91.54761904761905\n",
      "Training Loss per 5000 steps: 0.30892591190487856\n",
      "Training Accuracy per 5000 steps: 91.5576694411415\n",
      "Training Loss per 5000 steps: 0.3086199955340041\n",
      "Training Accuracy per 5000 steps: 91.56769596199526\n",
      "Training Loss per 5000 steps: 0.3082900485333192\n",
      "Training Accuracy per 5000 steps: 91.57769869513642\n",
      "Training Loss per 5000 steps: 0.3079613705322376\n",
      "Training Accuracy per 5000 steps: 91.58767772511848\n",
      "Training Loss per 5000 steps: 0.30896183185433673\n",
      "Training Accuracy per 5000 steps: 91.5680473372781\n",
      "Training Loss per 5000 steps: 0.30878209148425756\n",
      "Training Accuracy per 5000 steps: 91.57801418439716\n",
      "Training Loss per 5000 steps: 0.3084610396217656\n",
      "Training Accuracy per 5000 steps: 91.58795749704841\n",
      "Training Loss per 5000 steps: 0.30883387753515906\n",
      "Training Accuracy per 5000 steps: 91.56839622641509\n",
      "Training Loss per 5000 steps: 0.3085048395199142\n",
      "Training Accuracy per 5000 steps: 91.57832744405182\n",
      "Training Loss per 5000 steps: 0.3082210795592297\n",
      "Training Accuracy per 5000 steps: 91.58823529411765\n",
      "Training Loss per 5000 steps: 0.30791281804674414\n",
      "Training Accuracy per 5000 steps: 91.59811985898942\n",
      "Training Loss per 5000 steps: 0.3075945050304782\n",
      "Training Accuracy per 5000 steps: 91.60798122065728\n",
      "Training Loss per 5000 steps: 0.3072917860357259\n",
      "Training Accuracy per 5000 steps: 91.61781946072685\n",
      "Training Loss per 5000 steps: 0.30695022757228385\n",
      "Training Accuracy per 5000 steps: 91.62763466042155\n",
      "Training Loss per 5000 steps: 0.30660834705772977\n",
      "Training Accuracy per 5000 steps: 91.6374269005848\n",
      "Training Loss per 5000 steps: 0.30626943220806563\n",
      "Training Accuracy per 5000 steps: 91.64719626168224\n",
      "Training Loss per 5000 steps: 0.30628109135520465\n",
      "Training Accuracy per 5000 steps: 91.62777129521587\n",
      "Training Loss per 5000 steps: 0.30758233188536444\n",
      "Training Accuracy per 5000 steps: 91.60839160839161\n",
      "Training Loss per 5000 steps: 0.30724050358536287\n",
      "Training Accuracy per 5000 steps: 91.61816065192083\n",
      "Training Loss per 5000 steps: 0.3069081396102732\n",
      "Training Accuracy per 5000 steps: 91.62790697674419\n",
      "Training Loss per 5000 steps: 0.3065985960347208\n",
      "Training Accuracy per 5000 steps: 91.63763066202091\n",
      "Training Loss per 5000 steps: 0.30687794468977125\n",
      "Training Accuracy per 5000 steps: 91.61832946635731\n",
      "Training Loss per 5000 steps: 0.3066014248126662\n",
      "Training Accuracy per 5000 steps: 91.62804171494786\n",
      "Training Loss per 5000 steps: 0.3076523302868871\n",
      "Training Accuracy per 5000 steps: 91.60879629629629\n",
      "Training Loss per 5000 steps: 0.3073238982049199\n",
      "Training Accuracy per 5000 steps: 91.61849710982659\n",
      "Training Loss per 5000 steps: 0.3069888648832987\n",
      "Training Accuracy per 5000 steps: 91.62817551963049\n",
      "Training Loss per 5000 steps: 0.30665834388784813\n",
      "Training Accuracy per 5000 steps: 91.63783160322953\n",
      "Training Loss per 5000 steps: 0.30631983093059006\n",
      "Training Accuracy per 5000 steps: 91.64746543778801\n",
      "Training Loss per 5000 steps: 0.30608045108299126\n",
      "Training Accuracy per 5000 steps: 91.65707710011507\n",
      "Training Loss per 5000 steps: 0.3062437457599859\n",
      "Training Accuracy per 5000 steps: 91.63793103448276\n",
      "Training Loss per 5000 steps: 0.3061989835291314\n",
      "Training Accuracy per 5000 steps: 91.61882893226176\n",
      "Training Loss per 5000 steps: 0.30586200894001836\n",
      "Training Accuracy per 5000 steps: 91.62844036697248\n",
      "Training Loss per 5000 steps: 0.30553631333448633\n",
      "Training Accuracy per 5000 steps: 91.63802978235968\n",
      "Training Loss per 5000 steps: 0.3052141160514521\n",
      "Training Accuracy per 5000 steps: 91.64759725400458\n",
      "Training Loss per 5000 steps: 0.3049080984336989\n",
      "Training Accuracy per 5000 steps: 91.65714285714286\n",
      "Training Loss per 5000 steps: 0.3045800804547523\n",
      "Training Accuracy per 5000 steps: 91.66666666666667\n",
      "Training Loss per 5000 steps: 0.3043188817009007\n",
      "Training Accuracy per 5000 steps: 91.67616875712656\n",
      "Training Loss per 5000 steps: 0.30399759574570523\n",
      "Training Accuracy per 5000 steps: 91.68564920273349\n",
      "Training Loss per 5000 steps: 0.30367226725543794\n",
      "Training Accuracy per 5000 steps: 91.69510807736064\n",
      "Training Loss per 5000 steps: 0.30342104154511945\n",
      "Training Accuracy per 5000 steps: 91.70454545454545\n",
      "Training Loss per 5000 steps: 0.3036274984457808\n",
      "Training Accuracy per 5000 steps: 91.6855845629966\n",
      "Training Loss per 5000 steps: 0.3033529841731769\n",
      "Training Accuracy per 5000 steps: 91.69501133786848\n",
      "Training Loss per 5000 steps: 0.3030224058857253\n",
      "Training Accuracy per 5000 steps: 91.7044167610419\n",
      "Training Loss per 5000 steps: 0.30269794950337564\n",
      "Training Accuracy per 5000 steps: 91.71380090497738\n",
      "Training Loss per 5000 steps: 0.3023720205482622\n",
      "Training Accuracy per 5000 steps: 91.7231638418079\n",
      "Training Loss per 5000 steps: 0.3020467573680135\n",
      "Training Accuracy per 5000 steps: 91.73250564334086\n",
      "Training Loss per 5000 steps: 0.30171993909048794\n",
      "Training Accuracy per 5000 steps: 91.74182638105975\n",
      "Training Loss per 5000 steps: 0.3014349917889581\n",
      "Training Accuracy per 5000 steps: 91.75112612612612\n",
      "Training Loss per 5000 steps: 0.301113352196289\n",
      "Training Accuracy per 5000 steps: 91.76040494938132\n",
      "Training Loss per 5000 steps: 0.30131922334097744\n",
      "Training Accuracy per 5000 steps: 91.74157303370787\n",
      "Training Loss per 5000 steps: 0.30100270273533936\n",
      "Training Accuracy per 5000 steps: 91.75084175084174\n",
      "Training Loss per 5000 steps: 0.3007342162157822\n",
      "Training Accuracy per 5000 steps: 91.76008968609865\n",
      "Training Loss per 5000 steps: 0.30041695915763283\n",
      "Training Accuracy per 5000 steps: 91.76931690929452\n",
      "Training Loss per 5000 steps: 0.3005359013787819\n",
      "Training Accuracy per 5000 steps: 91.75055928411633\n",
      "Training Loss per 5000 steps: 0.3002289927725852\n",
      "Training Accuracy per 5000 steps: 91.75977653631286\n",
      "Training Loss per 5000 steps: 0.29991075566275477\n",
      "Training Accuracy per 5000 steps: 91.76897321428571\n",
      "Training Loss per 5000 steps: 0.30107401456027694\n",
      "Training Accuracy per 5000 steps: 91.75027870680044\n",
      "Training Loss per 5000 steps: 0.30075825802579453\n",
      "Training Accuracy per 5000 steps: 91.75946547884188\n",
      "Training Loss per 5000 steps: 0.30059971968157506\n",
      "Training Accuracy per 5000 steps: 91.7686318131257\n",
      "Training Loss per 5000 steps: 0.3002814350349622\n",
      "Training Accuracy per 5000 steps: 91.77777777777777\n",
      "Training Loss per 5000 steps: 0.3001018298839474\n",
      "Training Accuracy per 5000 steps: 91.78690344062153\n",
      "Training Loss per 5000 steps: 0.29996422337814177\n",
      "Training Accuracy per 5000 steps: 91.7960088691796\n",
      "Training Loss per 5000 steps: 0.29965534815224404\n",
      "Training Accuracy per 5000 steps: 91.80509413067553\n",
      "Training Loss per 5000 steps: 0.2993383081310503\n",
      "Training Accuracy per 5000 steps: 91.8141592920354\n",
      "Training Loss per 5000 steps: 0.29927005800032647\n",
      "Training Accuracy per 5000 steps: 91.8232044198895\n",
      "Training Loss per 5000 steps: 0.2996399786095104\n",
      "Training Accuracy per 5000 steps: 91.80463576158941\n",
      "Training Loss per 5000 steps: 0.2993782859884999\n",
      "Training Accuracy per 5000 steps: 91.81367144432194\n",
      "Training Loss per 5000 steps: 0.29932379312013246\n",
      "Training Accuracy per 5000 steps: 91.79515418502203\n",
      "Training Loss per 5000 steps: 0.2994573495931702\n",
      "Training Accuracy per 5000 steps: 91.77667766776678\n",
      "Training Loss per 5000 steps: 0.29914932733729155\n",
      "Training Accuracy per 5000 steps: 91.78571428571429\n",
      "Training Loss per 5000 steps: 0.29891205394720305\n",
      "Training Accuracy per 5000 steps: 91.794731064764\n",
      "Training Loss per 5000 steps: 0.2986018020085778\n",
      "Training Accuracy per 5000 steps: 91.80372807017544\n",
      "Training Loss per 5000 steps: 0.2982890349001851\n",
      "Training Accuracy per 5000 steps: 91.81270536692223\n",
      "Training Loss per 5000 steps: 0.2980399881204268\n",
      "Training Accuracy per 5000 steps: 91.82166301969366\n",
      "Training Loss per 5000 steps: 0.29810775382943006\n",
      "Training Accuracy per 5000 steps: 91.80327868852459\n",
      "Training Loss per 5000 steps: 0.297835067285664\n",
      "Training Accuracy per 5000 steps: 91.8122270742358\n",
      "Training Loss per 5000 steps: 0.2975265851362142\n",
      "Training Accuracy per 5000 steps: 91.82115594329335\n",
      "Training Loss per 5000 steps: 0.2991361111891507\n",
      "Training Accuracy per 5000 steps: 91.7755991285403\n",
      "Training Loss per 5000 steps: 0.29886163289682033\n",
      "Training Accuracy per 5000 steps: 91.78454842219804\n",
      "Training Loss per 5000 steps: 0.298647852769405\n",
      "Training Accuracy per 5000 steps: 91.79347826086956\n",
      "Training Loss per 5000 steps: 0.2983507922376293\n",
      "Training Accuracy per 5000 steps: 91.80238870792617\n",
      "Training Loss per 5000 steps: 0.298098141914375\n",
      "Training Accuracy per 5000 steps: 91.81127982646422\n",
      "Training Loss per 5000 steps: 0.2977886164797024\n",
      "Training Accuracy per 5000 steps: 91.8201516793066\n",
      "Training Loss per 5000 steps: 0.29750663900417296\n",
      "Training Accuracy per 5000 steps: 91.82900432900433\n",
      "Training Loss per 5000 steps: 0.29766443230413103\n",
      "Training Accuracy per 5000 steps: 91.8108108108108\n",
      "Training Loss per 5000 steps: 0.2973869614037595\n",
      "Training Accuracy per 5000 steps: 91.81965442764579\n",
      "Training Loss per 5000 steps: 0.29713038422223564\n",
      "Training Accuracy per 5000 steps: 91.8284789644013\n",
      "Training Loss per 5000 steps: 0.29686948435846716\n",
      "Training Accuracy per 5000 steps: 91.83728448275862\n",
      "Training Loss per 5000 steps: 0.29661103790331195\n",
      "Training Accuracy per 5000 steps: 91.84607104413348\n",
      "Training Loss per 5000 steps: 0.296459721437385\n",
      "Training Accuracy per 5000 steps: 91.85483870967742\n",
      "Training Loss per 5000 steps: 0.29640564630578337\n",
      "Training Accuracy per 5000 steps: 91.83673469387755\n",
      "Training Loss per 5000 steps: 0.29613503667050295\n",
      "Training Accuracy per 5000 steps: 91.84549356223177\n",
      "Training Loss per 5000 steps: 0.2964027336851513\n",
      "Training Accuracy per 5000 steps: 91.82743837084674\n",
      "Training Loss per 5000 steps: 0.2961029460857078\n",
      "Training Accuracy per 5000 steps: 91.83618843683084\n",
      "Training Loss per 5000 steps: 0.2958104818103148\n",
      "Training Accuracy per 5000 steps: 91.84491978609626\n",
      "Training Loss per 5000 steps: 0.29566964126613915\n",
      "Training Accuracy per 5000 steps: 91.85363247863248\n",
      "Training Loss per 5000 steps: 0.2953728564489104\n",
      "Training Accuracy per 5000 steps: 91.8623265741729\n",
      "Training Loss per 5000 steps: 0.2950761222910049\n",
      "Training Accuracy per 5000 steps: 91.87100213219617\n",
      "Training Loss per 5000 steps: 0.29477896473317927\n",
      "Training Accuracy per 5000 steps: 91.87965921192759\n",
      "Training Loss per 5000 steps: 0.2945479222295925\n",
      "Training Accuracy per 5000 steps: 91.88829787234043\n",
      "Training Loss per 5000 steps: 0.29424773041439994\n",
      "Training Accuracy per 5000 steps: 91.89691817215729\n",
      "Training Loss per 5000 steps: 0.29395624711880197\n",
      "Training Accuracy per 5000 steps: 91.90552016985139\n",
      "Training Loss per 5000 steps: 0.29368056697239614\n",
      "Training Accuracy per 5000 steps: 91.91410392364793\n",
      "Training Loss per 5000 steps: 0.2933823821111188\n",
      "Training Accuracy per 5000 steps: 91.92266949152543\n",
      "Training Loss per 5000 steps: 0.29319966974318346\n",
      "Training Accuracy per 5000 steps: 91.93121693121694\n",
      "Training Loss per 5000 steps: 0.2940800114324116\n",
      "Training Accuracy per 5000 steps: 91.91331923890064\n",
      "Training Loss per 5000 steps: 0.29379999702117765\n",
      "Training Accuracy per 5000 steps: 91.92185850052799\n",
      "Training Loss per 5000 steps: 0.2935174252319185\n",
      "Training Accuracy per 5000 steps: 91.93037974683544\n",
      "Training Loss per 5000 steps: 0.2932222841191719\n",
      "Training Accuracy per 5000 steps: 91.93888303477344\n",
      "Training Loss per 5000 steps: 0.2929273528762554\n",
      "Training Accuracy per 5000 steps: 91.94736842105263\n",
      "Training Loss per 5000 steps: 0.29263892232760647\n",
      "Training Accuracy per 5000 steps: 91.95583596214512\n",
      "Training Loss per 5000 steps: 0.2923465126415151\n",
      "Training Accuracy per 5000 steps: 91.96428571428571\n",
      "Training Loss per 5000 steps: 0.2921301586893539\n",
      "Training Accuracy per 5000 steps: 91.97271773347325\n",
      "Training Loss per 5000 steps: 0.2918640246636291\n",
      "Training Accuracy per 5000 steps: 91.98113207547169\n",
      "Training Loss per 5000 steps: 0.2932899088340358\n",
      "Training Accuracy per 5000 steps: 91.96335078534031\n",
      "Training Loss per 5000 steps: 0.29300137303640994\n",
      "Training Accuracy per 5000 steps: 91.97175732217573\n",
      "Training Loss per 5000 steps: 0.29277592434868016\n",
      "Training Accuracy per 5000 steps: 91.98014629049112\n",
      "Training Loss per 5000 steps: 0.2925110230200957\n",
      "Training Accuracy per 5000 steps: 91.98851774530272\n",
      "Training Loss per 5000 steps: 0.2922518828464235\n",
      "Training Accuracy per 5000 steps: 91.99687174139729\n",
      "Training Loss per 5000 steps: 0.2934531621237208\n",
      "Training Accuracy per 5000 steps: 91.97916666666667\n",
      "Training Loss per 5000 steps: 0.29316670184278554\n",
      "Training Accuracy per 5000 steps: 91.98751300728408\n",
      "Training Loss per 5000 steps: 0.29288764705246395\n",
      "Training Accuracy per 5000 steps: 91.995841995842\n",
      "Training Loss per 5000 steps: 0.29259955746374117\n",
      "Training Accuracy per 5000 steps: 92.00415368639668\n",
      "Training Loss per 5000 steps: 0.29231474257684875\n",
      "Training Accuracy per 5000 steps: 92.01244813278008\n",
      "Training Loss per 5000 steps: 0.2922667755335183\n",
      "Training Accuracy per 5000 steps: 91.99481865284974\n",
      "Training Loss per 5000 steps: 0.2922245057937969\n",
      "Training Accuracy per 5000 steps: 92.0031055900621\n",
      "Training Loss per 5000 steps: 0.291941332106884\n",
      "Training Accuracy per 5000 steps: 92.01137538779732\n",
      "Training Loss per 5000 steps: 0.2916678931758439\n",
      "Training Accuracy per 5000 steps: 92.01962809917356\n",
      "Training Loss per 5000 steps: 0.29137584171570274\n",
      "Training Accuracy per 5000 steps: 92.02786377708978\n",
      "Training Loss per 5000 steps: 0.29111732804533286\n",
      "Training Accuracy per 5000 steps: 92.0360824742268\n",
      "Training Loss per 5000 steps: 0.29106174126348\n",
      "Training Accuracy per 5000 steps: 92.01853759011328\n",
      "Training Loss per 5000 steps: 0.290789485542073\n",
      "Training Accuracy per 5000 steps: 92.02674897119341\n",
      "Training Loss per 5000 steps: 0.29050506658005965\n",
      "Training Accuracy per 5000 steps: 92.0349434737924\n",
      "Training Loss per 5000 steps: 0.29025027568278033\n",
      "Training Accuracy per 5000 steps: 92.04312114989733\n",
      "Training Loss per 5000 steps: 0.2899936394020915\n",
      "Training Accuracy per 5000 steps: 92.05128205128206\n",
      "Training Loss per 5000 steps: 0.28971637422161667\n",
      "Training Accuracy per 5000 steps: 92.05942622950819\n",
      "Training Loss per 5000 steps: 0.2894999007937501\n",
      "Training Accuracy per 5000 steps: 92.06755373592631\n",
      "Training Loss per 5000 steps: 0.28922020185811176\n",
      "Training Accuracy per 5000 steps: 92.07566462167689\n",
      "Training Loss per 5000 steps: 0.288944675851579\n",
      "Training Accuracy per 5000 steps: 92.08375893769153\n",
      "Training Loss per 5000 steps: 0.2886652653694761\n",
      "Training Accuracy per 5000 steps: 92.09183673469387\n",
      "Training Loss per 5000 steps: 0.28839103814316996\n",
      "Training Accuracy per 5000 steps: 92.09989806320081\n",
      "Training Loss per 5000 steps: 0.28811551520672996\n",
      "Training Accuracy per 5000 steps: 92.10794297352342\n",
      "Training Loss per 5000 steps: 0.28783508214061193\n",
      "Training Accuracy per 5000 steps: 92.11597151576805\n",
      "Training Loss per 5000 steps: 0.28763898685139155\n",
      "Training Accuracy per 5000 steps: 92.1239837398374\n",
      "Training Loss per 5000 steps: 0.28735858924062874\n",
      "Training Accuracy per 5000 steps: 92.13197969543147\n",
      "Training Loss per 5000 steps: 0.28708223007150885\n",
      "Training Accuracy per 5000 steps: 92.13995943204868\n",
      "Training Loss per 5000 steps: 0.28680535258674333\n",
      "Training Accuracy per 5000 steps: 92.14792299898683\n",
      "Training Loss per 5000 steps: 0.2867532081389234\n",
      "Training Accuracy per 5000 steps: 92.13056680161944\n",
      "Training Loss per 5000 steps: 0.2880329344161721\n",
      "Training Accuracy per 5000 steps: 92.11324570273003\n",
      "Training Loss per 5000 steps: 0.2877634517284054\n",
      "Training Accuracy per 5000 steps: 92.12121212121212\n",
      "Training Loss per 5000 steps: 0.28754720402699185\n",
      "Training Accuracy per 5000 steps: 92.12916246215943\n",
      "Training Loss per 5000 steps: 0.2872739800556202\n",
      "Training Accuracy per 5000 steps: 92.13709677419355\n",
      "Training Loss per 5000 steps: 0.28699719489993825\n",
      "Training Accuracy per 5000 steps: 92.14501510574019\n",
      "Training Loss per 5000 steps: 0.2877686648972527\n",
      "Training Accuracy per 5000 steps: 92.12776659959759\n",
      "Training Loss per 5000 steps: 0.2874928184433948\n",
      "Training Accuracy per 5000 steps: 92.1356783919598\n",
      "Training Loss per 5000 steps: 0.2872508755271663\n",
      "Training Accuracy per 5000 steps: 92.14357429718875\n",
      "Training Loss per 5000 steps: 0.2870074707095548\n",
      "Training Accuracy per 5000 steps: 92.15145436308927\n",
      "Training Loss per 5000 steps: 0.2867849404365571\n",
      "Training Accuracy per 5000 steps: 92.15931863727455\n",
      "Training Loss per 5000 steps: 0.28654125315492396\n",
      "Training Accuracy per 5000 steps: 92.16716716716716\n",
      "Training Loss per 5000 steps: 0.286280396586284\n",
      "Training Accuracy per 5000 steps: 92.175\n",
      "Training Loss per 5000 steps: 0.28602142834460936\n",
      "Training Accuracy per 5000 steps: 92.18281718281719\n",
      "Training Loss per 5000 steps: 0.28583042792782576\n",
      "Training Accuracy per 5000 steps: 92.19061876247505\n",
      "Training Loss per 5000 steps: 0.28556159048797125\n",
      "Training Accuracy per 5000 steps: 92.19840478564308\n",
      "Training Loss per 5000 steps: 0.28530653042763293\n",
      "Training Accuracy per 5000 steps: 92.20617529880478\n",
      "Training Loss per 5000 steps: 0.28503498111235265\n",
      "Training Accuracy per 5000 steps: 92.2139303482587\n",
      "Training Loss per 5000 steps: 0.2849326862140779\n",
      "Training Accuracy per 5000 steps: 92.22166998011929\n",
      "Training Loss per 5000 steps: 0.2846633166244925\n",
      "Training Accuracy per 5000 steps: 92.22939424031777\n",
      "Training Loss per 5000 steps: 0.2843999121899879\n",
      "Training Accuracy per 5000 steps: 92.23710317460318\n",
      "Training Loss per 5000 steps: 0.284368949836144\n",
      "Training Accuracy per 5000 steps: 92.22001982160555\n",
      "Training Loss per 5000 steps: 0.28490807524707057\n",
      "Training Accuracy per 5000 steps: 92.20297029702971\n",
      "Training Loss per 5000 steps: 0.2847561283086811\n",
      "Training Accuracy per 5000 steps: 92.2106824925816\n",
      "Training Loss per 5000 steps: 0.28457347199821426\n",
      "Training Accuracy per 5000 steps: 92.21837944664031\n",
      "Training Loss per 5000 steps: 0.2845323813264603\n",
      "Training Accuracy per 5000 steps: 92.22606120434354\n",
      "Training Loss per 5000 steps: 0.2842690381322504\n",
      "Training Accuracy per 5000 steps: 92.23372781065089\n",
      "Training Loss per 5000 steps: 0.28400451253750936\n",
      "Training Accuracy per 5000 steps: 92.24137931034483\n",
      "Training Loss per 5000 steps: 0.2838341688396277\n",
      "Training Accuracy per 5000 steps: 92.2490157480315\n",
      "Training Loss per 5000 steps: 0.28356744694608677\n",
      "Training Accuracy per 5000 steps: 92.2566371681416\n",
      "Training Loss per 5000 steps: 0.2842673089754107\n",
      "Training Accuracy per 5000 steps: 92.23968565815323\n",
      "Training Loss per 5000 steps: 0.28402040564474285\n",
      "Training Accuracy per 5000 steps: 92.24730127576055\n",
      "Training Loss per 5000 steps: 0.284778624661632\n",
      "Training Accuracy per 5000 steps: 92.23039215686275\n",
      "Training Loss per 5000 steps: 0.28551212301063666\n",
      "Training Accuracy per 5000 steps: 92.21351616062684\n",
      "Training Loss per 5000 steps: 0.2852677990921177\n",
      "Training Accuracy per 5000 steps: 92.2211350293542\n",
      "Training Loss per 5000 steps: 0.2856060338651767\n",
      "Training Accuracy per 5000 steps: 92.20430107526882\n",
      "Training Loss per 5000 steps: 0.285586275791502\n",
      "Training Accuracy per 5000 steps: 92.1875\n",
      "Training Loss per 5000 steps: 0.28534786906002496\n",
      "Training Accuracy per 5000 steps: 92.1951219512195\n",
      "Training Loss per 5000 steps: 0.2857449080987487\n",
      "Training Accuracy per 5000 steps: 92.17836257309942\n",
      "Training Loss per 5000 steps: 0.2858656249421684\n",
      "Training Accuracy per 5000 steps: 92.16163583252191\n",
      "Training Loss per 5000 steps: 0.2856142652884184\n",
      "Training Accuracy per 5000 steps: 92.1692607003891\n",
      "Training Loss per 5000 steps: 0.28710198214873167\n",
      "Training Accuracy per 5000 steps: 92.12827988338192\n",
      "Training Loss per 5000 steps: 0.286895302191232\n",
      "Training Accuracy per 5000 steps: 92.13592233009709\n",
      "Training Loss per 5000 steps: 0.28666707002905595\n",
      "Training Accuracy per 5000 steps: 92.14354995150339\n",
      "Training Loss per 5000 steps: 0.28641197659696144\n",
      "Training Accuracy per 5000 steps: 92.15116279069767\n",
      "Training Loss per 5000 steps: 0.2861722372941103\n",
      "Training Accuracy per 5000 steps: 92.15876089060987\n",
      "Training Loss per 5000 steps: 0.2859395071071513\n",
      "Training Accuracy per 5000 steps: 92.16634429400386\n",
      "Training Loss per 5000 steps: 0.28569302614688297\n",
      "Training Accuracy per 5000 steps: 92.17391304347827\n",
      "Training Loss per 5000 steps: 0.2859422586189446\n",
      "Training Accuracy per 5000 steps: 92.1573359073359\n",
      "Training Loss per 5000 steps: 0.28581185596873815\n",
      "Training Accuracy per 5000 steps: 92.1648987463838\n",
      "Training Loss per 5000 steps: 0.2856581147889354\n",
      "Training Accuracy per 5000 steps: 92.17244701348747\n",
      "Training Loss per 5000 steps: 0.2863657176171907\n",
      "Training Accuracy per 5000 steps: 92.15591915303176\n",
      "Training Loss per 5000 steps: 0.28644198895288775\n",
      "Training Accuracy per 5000 steps: 92.13942307692308\n",
      "Training Loss per 5000 steps: 0.2861952498469298\n",
      "Training Accuracy per 5000 steps: 92.14697406340058\n",
      "Training Loss per 5000 steps: 0.2861693632679915\n",
      "Training Accuracy per 5000 steps: 92.13051823416507\n",
      "Training Loss per 5000 steps: 0.28594335890084877\n",
      "Training Accuracy per 5000 steps: 92.13806327900288\n",
      "Training Loss per 5000 steps: 0.2857488436817095\n",
      "Training Accuracy per 5000 steps: 92.1455938697318\n",
      "Training Loss per 5000 steps: 0.28552151790075897\n",
      "Training Accuracy per 5000 steps: 92.1531100478469\n",
      "Training Loss per 5000 steps: 0.28555607453814885\n",
      "Training Accuracy per 5000 steps: 92.13671128107075\n",
      "Training Loss per 5000 steps: 0.2854032801551373\n",
      "Training Accuracy per 5000 steps: 92.14422158548233\n",
      "Training Loss per 5000 steps: 0.28559930973778697\n",
      "Training Accuracy per 5000 steps: 92.12786259541984\n",
      "Training Loss per 5000 steps: 0.28537509448336235\n",
      "Training Accuracy per 5000 steps: 92.1353670162059\n",
      "Training Loss per 5000 steps: 0.28516186199372723\n",
      "Training Accuracy per 5000 steps: 92.14285714285714\n",
      "Training Loss per 5000 steps: 0.284935124689885\n",
      "Training Accuracy per 5000 steps: 92.15033301617507\n",
      "Training Loss per 5000 steps: 0.2847345949897805\n",
      "Training Accuracy per 5000 steps: 92.15779467680608\n",
      "Training Loss per 5000 steps: 0.28466607622115925\n",
      "Training Accuracy per 5000 steps: 92.16524216524216\n",
      "Training Loss per 5000 steps: 0.2844398388395217\n",
      "Training Accuracy per 5000 steps: 92.17267552182163\n",
      "Training Loss per 5000 steps: 0.2842138413816549\n",
      "Training Accuracy per 5000 steps: 92.18009478672985\n",
      "Training Loss per 5000 steps: 0.2840460724952264\n",
      "Training Accuracy per 5000 steps: 92.1875\n",
      "Training Loss per 5000 steps: 0.2837936785072088\n",
      "Training Accuracy per 5000 steps: 92.19489120151371\n",
      "Training Loss per 5000 steps: 0.28355269158163965\n",
      "Training Accuracy per 5000 steps: 92.2022684310019\n",
      "Training Loss per 5000 steps: 0.2833760166813367\n",
      "Training Accuracy per 5000 steps: 92.20963172804532\n",
      "Training Loss per 5000 steps: 0.2831322353526528\n",
      "Training Accuracy per 5000 steps: 92.21698113207547\n",
      "Training Loss per 5000 steps: 0.2828949975020109\n",
      "Training Accuracy per 5000 steps: 92.22431668237512\n",
      "Training Loss per 5000 steps: 0.282645780552817\n",
      "Training Accuracy per 5000 steps: 92.2316384180791\n",
      "Training Loss per 5000 steps: 0.2823920485949948\n",
      "Training Accuracy per 5000 steps: 92.23894637817497\n",
      "Training Loss per 5000 steps: 0.28242123815497117\n",
      "Training Accuracy per 5000 steps: 92.22274436090225\n",
      "Training Loss per 5000 steps: 0.28217483689142786\n",
      "Training Accuracy per 5000 steps: 92.2300469483568\n",
      "Training Loss per 5000 steps: 0.2820870574951647\n",
      "Training Accuracy per 5000 steps: 92.23733583489681\n",
      "Training Loss per 5000 steps: 0.28187194411601824\n",
      "Training Accuracy per 5000 steps: 92.24461105904405\n",
      "Training Loss per 5000 steps: 0.28259794227953317\n",
      "Training Accuracy per 5000 steps: 92.22846441947566\n",
      "Training Loss per 5000 steps: 0.2823477019220451\n",
      "Training Accuracy per 5000 steps: 92.23573433115061\n",
      "Training Loss per 5000 steps: 0.28210899977135323\n",
      "Training Accuracy per 5000 steps: 92.24299065420561\n",
      "Training Loss per 5000 steps: 0.2818677657249929\n",
      "Training Accuracy per 5000 steps: 92.25023342670401\n",
      "Training Loss per 5000 steps: 0.2816208704711814\n",
      "Training Accuracy per 5000 steps: 92.25746268656717\n",
      "Training Loss per 5000 steps: 0.2813718314017868\n",
      "Training Accuracy per 5000 steps: 92.26467847157502\n",
      "Training Loss per 5000 steps: 0.2811277387776452\n",
      "Training Accuracy per 5000 steps: 92.27188081936686\n",
      "Training Loss per 5000 steps: 0.2808843310757778\n",
      "Training Accuracy per 5000 steps: 92.27906976744185\n",
      "Training Loss per 5000 steps: 0.2806365134646306\n",
      "Training Accuracy per 5000 steps: 92.28624535315986\n",
      "Training Loss per 5000 steps: 0.28057317990989544\n",
      "Training Accuracy per 5000 steps: 92.27019498607243\n",
      "Training Loss per 5000 steps: 0.2808038194833154\n",
      "Training Accuracy per 5000 steps: 92.25417439703153\n",
      "Training Loss per 5000 steps: 0.2809272897408186\n",
      "Training Accuracy per 5000 steps: 92.23818350324375\n",
      "Training Loss per 5000 steps: 0.28068353303414945\n",
      "Training Accuracy per 5000 steps: 92.24537037037037\n",
      "Training Loss per 5000 steps: 0.2804448937042638\n",
      "Training Accuracy per 5000 steps: 92.25254394079556\n",
      "Training Loss per 5000 steps: 0.28024115410759054\n",
      "Training Accuracy per 5000 steps: 92.25970425138632\n",
      "Training Loss per 5000 steps: 0.2800011772615964\n",
      "Training Accuracy per 5000 steps: 92.2668513388735\n",
      "Training Loss per 5000 steps: 0.279800779649273\n",
      "Training Accuracy per 5000 steps: 92.2739852398524\n",
      "Training Loss per 5000 steps: 0.27955752590645433\n",
      "Training Accuracy per 5000 steps: 92.28110599078342\n",
      "Training Loss per 5000 steps: 0.2793160732363867\n",
      "Training Accuracy per 5000 steps: 92.28821362799263\n",
      "Training Loss per 5000 steps: 0.2792104406131512\n",
      "Training Accuracy per 5000 steps: 92.29530818767249\n",
      "Training Loss per 5000 steps: 0.27897214083607064\n",
      "Training Accuracy per 5000 steps: 92.30238970588235\n",
      "Training Loss per 5000 steps: 0.2787751826997316\n",
      "Training Accuracy per 5000 steps: 92.30945821854912\n",
      "Training Loss per 5000 steps: 0.2785510279315122\n",
      "Training Accuracy per 5000 steps: 92.31651376146789\n",
      "Training Loss per 5000 steps: 0.27830711733481545\n",
      "Training Accuracy per 5000 steps: 92.32355637030247\n",
      "Training Loss per 5000 steps: 0.27817906638276474\n",
      "Training Accuracy per 5000 steps: 92.33058608058609\n",
      "Training Loss per 5000 steps: 0.2779453201935186\n",
      "Training Accuracy per 5000 steps: 92.33760292772186\n",
      "Training Loss per 5000 steps: 0.2783996832039432\n",
      "Training Accuracy per 5000 steps: 92.3217550274223\n",
      "Training Loss per 5000 steps: 0.27819925480560487\n",
      "Training Accuracy per 5000 steps: 92.32876712328768\n",
      "Training Loss per 5000 steps: 0.27796119171021383\n",
      "Training Accuracy per 5000 steps: 92.33576642335767\n",
      "Training Loss per 5000 steps: 0.27772253659811064\n",
      "Training Accuracy per 5000 steps: 92.34275296262534\n",
      "Training Loss per 5000 steps: 0.27805829623999084\n",
      "Training Accuracy per 5000 steps: 92.32695810564663\n",
      "Training Loss per 5000 steps: 0.2778213947843608\n",
      "Training Accuracy per 5000 steps: 92.33393994540491\n",
      "Training Loss per 5000 steps: 0.2777932283629409\n",
      "Training Accuracy per 5000 steps: 92.31818181818181\n",
      "Training Loss per 5000 steps: 0.2775818683235795\n",
      "Training Accuracy per 5000 steps: 92.32515894641236\n",
      "Training Loss per 5000 steps: 0.2773470481707226\n",
      "Training Accuracy per 5000 steps: 92.33212341197822\n",
      "Training Loss per 5000 steps: 0.2772489554567072\n",
      "Training Accuracy per 5000 steps: 92.33907524932003\n",
      "Training Loss per 5000 steps: 0.27701930457007384\n",
      "Training Accuracy per 5000 steps: 92.34601449275362\n",
      "Training Loss per 5000 steps: 0.27681357653145744\n",
      "Training Accuracy per 5000 steps: 92.3529411764706\n",
      "Training Loss per 5000 steps: 0.2767205616959436\n",
      "Training Accuracy per 5000 steps: 92.35985533453888\n",
      "Training Loss per 5000 steps: 0.2765930131601057\n",
      "Training Accuracy per 5000 steps: 92.36675700090335\n",
      "Training Loss per 5000 steps: 0.27714954631567834\n",
      "Training Accuracy per 5000 steps: 92.35108303249098\n",
      "Training Loss per 5000 steps: 0.2769164432829449\n",
      "Training Accuracy per 5000 steps: 92.35798016230838\n",
      "Training Loss per 5000 steps: 0.27707326423192213\n",
      "Training Accuracy per 5000 steps: 92.31981981981981\n",
      "Training Loss per 5000 steps: 0.2770436728862722\n",
      "Training Accuracy per 5000 steps: 92.3042304230423\n",
      "Training Loss per 5000 steps: 0.2768096139197231\n",
      "Training Accuracy per 5000 steps: 92.31115107913669\n",
      "Training Loss per 5000 steps: 0.2775051599227876\n",
      "Training Accuracy per 5000 steps: 92.29559748427673\n",
      "Training Loss per 5000 steps: 0.2781059302789238\n",
      "Training Accuracy per 5000 steps: 92.28007181328546\n",
      "Training Loss per 5000 steps: 0.27796607582408084\n",
      "Training Accuracy per 5000 steps: 92.28699551569507\n",
      "Training Loss per 5000 steps: 0.27774107432089695\n",
      "Training Accuracy per 5000 steps: 92.29390681003584\n",
      "Training Loss per 5000 steps: 0.27786693281028\n",
      "Training Accuracy per 5000 steps: 92.27842435094001\n",
      "Training Loss per 5000 steps: 0.27843993776836073\n",
      "Training Accuracy per 5000 steps: 92.26296958855099\n",
      "Training Loss per 5000 steps: 0.2782324128330646\n",
      "Training Accuracy per 5000 steps: 92.2698838248436\n",
      "Training Loss per 5000 steps: 0.278028454577517\n",
      "Training Accuracy per 5000 steps: 92.27678571428571\n",
      "Training Loss per 5000 steps: 0.27789549223736365\n",
      "Training Accuracy per 5000 steps: 92.28367528991971\n",
      "Training Loss per 5000 steps: 0.2776800331164902\n",
      "Training Accuracy per 5000 steps: 92.29055258467024\n",
      "Training Loss per 5000 steps: 0.2775437381120195\n",
      "Training Accuracy per 5000 steps: 92.29741763134462\n",
      "Training Loss per 5000 steps: 0.2773257199785874\n",
      "Training Accuracy per 5000 steps: 92.30427046263345\n",
      "Training Loss per 5000 steps: 0.2771591299763984\n",
      "Training Accuracy per 5000 steps: 92.31111111111112\n",
      "Training Loss per 5000 steps: 0.27695280080495754\n",
      "Training Accuracy per 5000 steps: 92.31793960923623\n",
      "Training Loss per 5000 steps: 0.2767461002420075\n",
      "Training Accuracy per 5000 steps: 92.32475598935227\n",
      "Training Loss per 5000 steps: 0.27657026137598517\n",
      "Training Accuracy per 5000 steps: 92.33156028368795\n",
      "Training Loss per 5000 steps: 0.2764103323727701\n",
      "Training Accuracy per 5000 steps: 92.33835252435784\n",
      "Training Loss per 5000 steps: 0.27711577181316976\n",
      "Training Accuracy per 5000 steps: 92.32300884955752\n",
      "Training Loss per 5000 steps: 0.27693545873582387\n",
      "Training Accuracy per 5000 steps: 92.32979664014147\n",
      "Training Loss per 5000 steps: 0.27671364113086233\n",
      "Training Accuracy per 5000 steps: 92.33657243816255\n",
      "Training Loss per 5000 steps: 0.27651191370057016\n",
      "Training Accuracy per 5000 steps: 92.34333627537511\n",
      "Training Loss per 5000 steps: 0.2763384595994609\n",
      "Training Accuracy per 5000 steps: 92.35008818342152\n",
      "Training Loss per 5000 steps: 0.2761252966918675\n",
      "Training Accuracy per 5000 steps: 92.3568281938326\n",
      "Training Loss per 5000 steps: 0.27604708541594575\n",
      "Training Accuracy per 5000 steps: 92.36355633802818\n",
      "Training Loss per 5000 steps: 0.27640771015695476\n",
      "Training Accuracy per 5000 steps: 92.34828496042216\n",
      "Training Loss per 5000 steps: 0.27624478225591337\n",
      "Training Accuracy per 5000 steps: 92.35500878734622\n",
      "Training Loss per 5000 steps: 0.27603690964075767\n",
      "Training Accuracy per 5000 steps: 92.36172080772607\n",
      "Training Loss per 5000 steps: 0.2758178090243682\n",
      "Training Accuracy per 5000 steps: 92.36842105263158\n",
      "Training Loss per 5000 steps: 0.2756003579288055\n",
      "Training Accuracy per 5000 steps: 92.37510955302366\n",
      "Training Loss per 5000 steps: 0.27541267420956295\n",
      "Training Accuracy per 5000 steps: 92.38178633975481\n",
      "Training Loss per 5000 steps: 0.2757886194290751\n",
      "Training Accuracy per 5000 steps: 92.3665791776028\n",
      "Training Loss per 5000 steps: 0.2755711745687311\n",
      "Training Accuracy per 5000 steps: 92.37325174825175\n",
      "Training Loss per 5000 steps: 0.27538142793580045\n",
      "Training Accuracy per 5000 steps: 92.37991266375546\n",
      "Training Loss per 5000 steps: 0.27521399905040445\n",
      "Training Accuracy per 5000 steps: 92.38656195462478\n",
      "Training Loss per 5000 steps: 0.27552801369077995\n",
      "Training Accuracy per 5000 steps: 92.37140366172625\n",
      "Training Loss per 5000 steps: 0.2753338570542673\n",
      "Training Accuracy per 5000 steps: 92.3780487804878\n",
      "Training Loss per 5000 steps: 0.2751161751448395\n",
      "Training Accuracy per 5000 steps: 92.38468233246302\n",
      "Training Loss per 5000 steps: 0.27522194641196857\n",
      "Training Accuracy per 5000 steps: 92.3695652173913\n",
      "Training Loss per 5000 steps: 0.2750142764220812\n",
      "Training Accuracy per 5000 steps: 92.37619461337967\n",
      "Training Loss per 5000 steps: 0.27484668150120545\n",
      "Training Accuracy per 5000 steps: 92.3828125\n",
      "Training Loss per 5000 steps: 0.2746765453276343\n",
      "Training Accuracy per 5000 steps: 92.38941890719862\n",
      "Training Loss per 5000 steps: 0.27445545095863205\n",
      "Training Accuracy per 5000 steps: 92.39601386481803\n",
      "Training Loss per 5000 steps: 0.27426783608845173\n",
      "Training Accuracy per 5000 steps: 92.40259740259741\n",
      "Training Loss per 5000 steps: 0.2740593328354977\n",
      "Training Accuracy per 5000 steps: 92.409169550173\n",
      "Training Loss per 5000 steps: 0.27386016421770243\n",
      "Training Accuracy per 5000 steps: 92.41573033707866\n",
      "Training Loss per 5000 steps: 0.2736647932626153\n",
      "Training Accuracy per 5000 steps: 92.42227979274611\n",
      "Training Loss per 5000 steps: 0.27368066520562734\n",
      "Training Accuracy per 5000 steps: 92.40724762726488\n",
      "Training Loss per 5000 steps: 0.2740627803767485\n",
      "Training Accuracy per 5000 steps: 92.39224137931035\n",
      "Training Loss per 5000 steps: 0.27387865828101515\n",
      "Training Accuracy per 5000 steps: 92.3987941429802\n",
      "Training Loss per 5000 steps: 0.2737154715979815\n",
      "Training Accuracy per 5000 steps: 92.40533562822719\n",
      "Training Loss per 5000 steps: 0.2734921969599887\n",
      "Training Accuracy per 5000 steps: 92.41186586414446\n",
      "Training Loss per 5000 steps: 0.27327844928037953\n",
      "Training Accuracy per 5000 steps: 92.41838487972508\n",
      "Training Loss per 5000 steps: 0.27305508418320457\n",
      "Training Accuracy per 5000 steps: 92.42489270386267\n",
      "Training Loss per 5000 steps: 0.27285017346560725\n",
      "Training Accuracy per 5000 steps: 92.43138936535163\n",
      "Training Loss per 5000 steps: 0.2726375710948561\n",
      "Training Accuracy per 5000 steps: 92.43787489288775\n",
      "Training Loss per 5000 steps: 0.2724231739093116\n",
      "Training Accuracy per 5000 steps: 92.4443493150685\n",
      "Training Loss per 5000 steps: 0.2725923247053786\n",
      "Training Accuracy per 5000 steps: 92.42942686056459\n",
      "Training Loss per 5000 steps: 0.2723794493004361\n",
      "Training Accuracy per 5000 steps: 92.43589743589743\n",
      "Training Loss per 5000 steps: 0.27216736965889887\n",
      "Training Accuracy per 5000 steps: 92.44235695986336\n",
      "Training Loss per 5000 steps: 0.271952002368861\n",
      "Training Accuracy per 5000 steps: 92.44880546075085\n",
      "Training Loss per 5000 steps: 0.27175654497518437\n",
      "Training Accuracy per 5000 steps: 92.45524296675192\n",
      "Training Loss per 5000 steps: 0.27162116816032555\n",
      "Training Accuracy per 5000 steps: 92.46166950596252\n",
      "Training Loss per 5000 steps: 0.27141627256144235\n",
      "Training Accuracy per 5000 steps: 92.46808510638297\n",
      "Training Loss per 5000 steps: 0.27188674862525064\n",
      "Training Accuracy per 5000 steps: 92.453231292517\n",
      "Training Loss per 5000 steps: 0.2723701450943681\n",
      "Training Accuracy per 5000 steps: 92.43840271877654\n",
      "Training Loss per 5000 steps: 0.2721507345082741\n",
      "Training Accuracy per 5000 steps: 92.44482173174873\n",
      "Training Loss per 5000 steps: 0.27195231950082727\n",
      "Training Accuracy per 5000 steps: 92.45122985581001\n",
      "Training Loss per 5000 steps: 0.2724346587331808\n",
      "Training Accuracy per 5000 steps: 92.4364406779661\n",
      "Training Loss per 5000 steps: 0.2722211642645041\n",
      "Training Accuracy per 5000 steps: 92.4428450465707\n",
      "Training Loss per 5000 steps: 0.2720212854071506\n",
      "Training Accuracy per 5000 steps: 92.44923857868021\n",
      "Training Loss per 5000 steps: 0.2723938370161294\n",
      "Training Accuracy per 5000 steps: 92.43448858833474\n",
      "Training Loss per 5000 steps: 0.27233582489566904\n",
      "Training Accuracy per 5000 steps: 92.44087837837837\n",
      "Training Loss per 5000 steps: 0.2721215596784079\n",
      "Training Accuracy per 5000 steps: 92.44725738396625\n",
      "Training Loss per 5000 steps: 0.27305305160937005\n",
      "Training Accuracy per 5000 steps: 92.43254637436762\n",
      "Training Loss per 5000 steps: 0.27313196961395286\n",
      "Training Accuracy per 5000 steps: 92.4178601516428\n",
      "Training Loss per 5000 steps: 0.2729637151127775\n",
      "Training Accuracy per 5000 steps: 92.42424242424242\n",
      "Training Loss per 5000 steps: 0.2733160895409017\n",
      "Training Accuracy per 5000 steps: 92.40958788898234\n",
      "Training Loss per 5000 steps: 0.2731117881419716\n",
      "Training Accuracy per 5000 steps: 92.41596638655462\n",
      "Training Loss per 5000 steps: 0.2729089285929685\n",
      "Training Accuracy per 5000 steps: 92.4223341729639\n",
      "Training Loss per 5000 steps: 0.27271483769412186\n",
      "Training Accuracy per 5000 steps: 92.42869127516778\n",
      "Training Loss per 5000 steps: 0.272512209940505\n",
      "Training Accuracy per 5000 steps: 92.43503772003353\n",
      "Training Loss per 5000 steps: 0.2722976301996415\n",
      "Training Accuracy per 5000 steps: 92.44137353433835\n",
      "Training Loss per 5000 steps: 0.27211160551382535\n",
      "Training Accuracy per 5000 steps: 92.44769874476988\n",
      "Training Loss per 5000 steps: 0.2719201519283276\n",
      "Training Accuracy per 5000 steps: 92.45401337792642\n",
      "Training Loss per 5000 steps: 0.27172199805145003\n",
      "Training Accuracy per 5000 steps: 92.46031746031746\n",
      "Training Loss per 5000 steps: 0.27253694446502447\n",
      "Training Accuracy per 5000 steps: 92.4457429048414\n",
      "Training Loss per 5000 steps: 0.27233136583156914\n",
      "Training Accuracy per 5000 steps: 92.45204336947457\n",
      "Training Loss per 5000 steps: 0.2721274986017185\n",
      "Training Accuracy per 5000 steps: 92.45833333333333\n",
      "Training Loss per 5000 steps: 0.27236718392475734\n",
      "Training Accuracy per 5000 steps: 92.44379683597002\n",
      "Training Loss per 5000 steps: 0.2721570691647017\n",
      "Training Accuracy per 5000 steps: 92.45008319467554\n",
      "Training Loss per 5000 steps: 0.27201808281691864\n",
      "Training Accuracy per 5000 steps: 92.45635910224439\n",
      "Training Loss per 5000 steps: 0.2719116245831156\n",
      "Training Accuracy per 5000 steps: 92.4626245847176\n",
      "Training Loss per 5000 steps: 0.2718486011144222\n",
      "Training Accuracy per 5000 steps: 92.4688796680498\n",
      "Training Loss per 5000 steps: 0.27198799385628963\n",
      "Training Accuracy per 5000 steps: 92.45439469320067\n",
      "Training Loss per 5000 steps: 0.2717858875269115\n",
      "Training Accuracy per 5000 steps: 92.46064623032312\n",
      "Training Loss per 5000 steps: 0.2716442832518184\n",
      "Training Accuracy per 5000 steps: 92.46688741721854\n",
      "Training Loss per 5000 steps: 0.27144548450916317\n",
      "Training Accuracy per 5000 steps: 92.47311827956989\n",
      "Training Loss per 5000 steps: 0.27126268664036285\n",
      "Training Accuracy per 5000 steps: 92.47933884297521\n",
      "Training Loss per 5000 steps: 0.27109345408198043\n",
      "Training Accuracy per 5000 steps: 92.48554913294798\n",
      "Training Loss per 5000 steps: 0.2709360057972187\n",
      "Training Accuracy per 5000 steps: 92.49174917491749\n",
      "Training Loss per 5000 steps: 0.271110469580575\n",
      "Training Accuracy per 5000 steps: 92.47732893652102\n",
      "Training Loss per 5000 steps: 0.27090161296583504\n",
      "Training Accuracy per 5000 steps: 92.4835255354201\n",
      "Training Loss per 5000 steps: 0.2706908554666572\n",
      "Training Accuracy per 5000 steps: 92.48971193415638\n",
      "Training Loss per 5000 steps: 0.2704822474242016\n",
      "Training Accuracy per 5000 steps: 92.49588815789474\n",
      "Training Loss per 5000 steps: 0.271354920885104\n",
      "Training Accuracy per 5000 steps: 92.48151191454396\n",
      "Training Loss per 5000 steps: 0.271142761833195\n",
      "Training Accuracy per 5000 steps: 92.48768472906404\n",
      "Training Loss per 5000 steps: 0.27093795540271187\n",
      "Training Accuracy per 5000 steps: 92.49384741591469\n",
      "Training Loss per 5000 steps: 0.2707296573847044\n",
      "Training Accuracy per 5000 steps: 92.5\n",
      "Training Loss per 5000 steps: 0.27072151886401463\n",
      "Training Accuracy per 5000 steps: 92.48566748566749\n",
      "Training Loss per 5000 steps: 0.2707184930796904\n",
      "Training Accuracy per 5000 steps: 92.47135842880523\n",
      "Training Loss per 5000 steps: 0.27050773237808284\n",
      "Training Accuracy per 5000 steps: 92.47751430907604\n",
      "Training Loss per 5000 steps: 0.2702967383437699\n",
      "Training Accuracy per 5000 steps: 92.48366013071896\n",
      "Training Loss per 5000 steps: 0.2700844740137762\n",
      "Training Accuracy per 5000 steps: 92.48979591836735\n",
      "Training Loss per 5000 steps: 0.26987359053599624\n",
      "Training Accuracy per 5000 steps: 92.49592169657423\n",
      "Training Loss per 5000 steps: 0.2697351289937347\n",
      "Training Accuracy per 5000 steps: 92.50203748981255\n",
      "Training Loss per 5000 steps: 0.26953482019399516\n",
      "Training Accuracy per 5000 steps: 92.50814332247558\n",
      "Training Loss per 5000 steps: 0.2698190982231493\n",
      "Training Accuracy per 5000 steps: 92.49389747762409\n",
      "Training Loss per 5000 steps: 0.2696860299430545\n",
      "Training Accuracy per 5000 steps: 92.5\n",
      "Training Loss per 5000 steps: 0.2694766776035536\n",
      "Training Accuracy per 5000 steps: 92.50609260763606\n",
      "Training Loss per 5000 steps: 0.26937287388061015\n",
      "Training Accuracy per 5000 steps: 92.51217532467533\n",
      "Training Loss per 5000 steps: 0.2691668810644652\n",
      "Training Accuracy per 5000 steps: 92.51824817518248\n",
      "Training Loss per 5000 steps: 0.26928412031406473\n",
      "Training Accuracy per 5000 steps: 92.50405186385737\n",
      "Training Loss per 5000 steps: 0.26908100073485963\n",
      "Training Accuracy per 5000 steps: 92.51012145748987\n",
      "Training Loss per 5000 steps: 0.2688907900008352\n",
      "Training Accuracy per 5000 steps: 92.51618122977347\n",
      "Training Loss per 5000 steps: 0.26869919369857015\n",
      "Training Accuracy per 5000 steps: 92.52223120452709\n",
      "Training Loss per 5000 steps: 0.268497407979517\n",
      "Training Accuracy per 5000 steps: 92.52827140549273\n",
      "Training Loss per 5000 steps: 0.2682990256239922\n",
      "Training Accuracy per 5000 steps: 92.53430185633576\n",
      "Training Loss per 5000 steps: 0.2681699137458758\n",
      "Training Accuracy per 5000 steps: 92.54032258064517\n",
      "Training Loss per 5000 steps: 0.26796320769230414\n",
      "Training Accuracy per 5000 steps: 92.54633360193392\n",
      "Training Loss per 5000 steps: 0.26939448526342086\n",
      "Training Accuracy per 5000 steps: 92.512077294686\n",
      "Training Loss per 5000 steps: 0.2691946620959264\n",
      "Training Accuracy per 5000 steps: 92.51810136765889\n",
      "Training Loss per 5000 steps: 0.26900565476752697\n",
      "Training Accuracy per 5000 steps: 92.52411575562701\n",
      "Training Loss per 5000 steps: 0.26967546792483954\n",
      "Training Accuracy per 5000 steps: 92.51004016064257\n",
      "Training Loss per 5000 steps: 0.2702052199160665\n",
      "Training Accuracy per 5000 steps: 92.4959871589085\n",
      "Training Loss per 5000 steps: 0.2699990802053915\n",
      "Training Accuracy per 5000 steps: 92.50200481154772\n",
      "Training Loss per 5000 steps: 0.2698133662925102\n",
      "Training Accuracy per 5000 steps: 92.50801282051282\n",
      "Training Loss per 5000 steps: 0.2696270310596598\n",
      "Training Accuracy per 5000 steps: 92.51401120896718\n",
      "Training Loss per 5000 steps: 0.2699650352701545\n",
      "Training Accuracy per 5000 steps: 92.48\n",
      "Training Loss per 5000 steps: 0.27026069080747434\n",
      "Training Accuracy per 5000 steps: 92.46602717825739\n",
      "Training Loss per 5000 steps: 0.27007336222890993\n",
      "Training Accuracy per 5000 steps: 92.47204472843451\n",
      "Training Loss per 5000 steps: 0.269887082984839\n",
      "Training Accuracy per 5000 steps: 92.4780526735834\n",
      "Training Loss per 5000 steps: 0.27023201060156077\n",
      "Training Accuracy per 5000 steps: 92.46411483253588\n",
      "Training Loss per 5000 steps: 0.27002918754084654\n",
      "Training Accuracy per 5000 steps: 92.47011952191235\n",
      "Training Loss per 5000 steps: 0.26990438287093\n",
      "Training Accuracy per 5000 steps: 92.47611464968153\n",
      "Training Loss per 5000 steps: 0.26969948368015134\n",
      "Training Accuracy per 5000 steps: 92.48210023866348\n",
      "Training Loss per 5000 steps: 0.26958727965424567\n",
      "Training Accuracy per 5000 steps: 92.48807631160572\n",
      "Training Loss per 5000 steps: 0.2693874397514073\n",
      "Training Accuracy per 5000 steps: 92.49404289118348\n",
      "Training Loss per 5000 steps: 0.2692341634028015\n",
      "Training Accuracy per 5000 steps: 92.5\n",
      "Training Loss per 5000 steps: 0.26903368894255986\n",
      "Training Accuracy per 5000 steps: 92.50594766058684\n",
      "Training Loss per 5000 steps: 0.2693007269936205\n",
      "Training Accuracy per 5000 steps: 92.49207606973059\n",
      "Training Loss per 5000 steps: 0.26910240529805585\n",
      "Training Accuracy per 5000 steps: 92.49802058590657\n",
      "Training Loss per 5000 steps: 0.2689645421865125\n",
      "Training Accuracy per 5000 steps: 92.50395569620254\n",
      "Training Loss per 5000 steps: 0.26878457582250176\n",
      "Training Accuracy per 5000 steps: 92.5098814229249\n",
      "Training Loss per 5000 steps: 0.2686097854954348\n",
      "Training Accuracy per 5000 steps: 92.51579778830964\n",
      "Training Loss per 5000 steps: 0.26841189153051415\n",
      "Training Accuracy per 5000 steps: 92.5217048145225\n",
      "Training Loss per 5000 steps: 0.268361417641305\n",
      "Training Accuracy per 5000 steps: 92.50788643533123\n",
      "Training Loss per 5000 steps: 0.26816024757493095\n",
      "Training Accuracy per 5000 steps: 92.51379038613081\n",
      "Training Loss per 5000 steps: 0.2679575016509419\n",
      "Training Accuracy per 5000 steps: 92.51968503937007\n",
      "Training Loss per 5000 steps: 0.26778686820455394\n",
      "Training Accuracy per 5000 steps: 92.5255704169945\n",
      "Training Loss per 5000 steps: 0.267788295289238\n",
      "Training Accuracy per 5000 steps: 92.51179245283019\n",
      "Training Loss per 5000 steps: 0.2676064552949962\n",
      "Training Accuracy per 5000 steps: 92.51767478397487\n",
      "Training Loss per 5000 steps: 0.26741046699793247\n",
      "Training Accuracy per 5000 steps: 92.52354788069074\n",
      "Training Loss per 5000 steps: 0.26721085059876537\n",
      "Training Accuracy per 5000 steps: 92.52941176470588\n",
      "Training Loss per 5000 steps: 0.2670283400538293\n",
      "Training Accuracy per 5000 steps: 92.53526645768025\n",
      "Training Loss per 5000 steps: 0.2671417590747051\n",
      "Training Accuracy per 5000 steps: 92.52153484729835\n",
      "Training Loss per 5000 steps: 0.2677110712071633\n",
      "Training Accuracy per 5000 steps: 92.50782472613459\n",
      "Training Loss per 5000 steps: 0.26804964015430727\n",
      "Training Accuracy per 5000 steps: 92.49413604378421\n",
      "Training Loss per 5000 steps: 0.2678558768791845\n",
      "Training Accuracy per 5000 steps: 92.5\n",
      "Training Loss per 5000 steps: 0.2676601155176119\n",
      "Training Accuracy per 5000 steps: 92.50585480093677\n",
      "Training Loss per 5000 steps: 0.2674621726973229\n",
      "Training Accuracy per 5000 steps: 92.51170046801872\n",
      "Training Loss per 5000 steps: 0.2673026032924408\n",
      "Training Accuracy per 5000 steps: 92.51753702260328\n",
      "Training Loss per 5000 steps: 0.26734123466829435\n",
      "Training Accuracy per 5000 steps: 92.5233644859813\n",
      "Training Loss per 5000 steps: 0.26717155934583237\n",
      "Training Accuracy per 5000 steps: 92.52918287937743\n",
      "Training Loss per 5000 steps: 0.2669857161367257\n",
      "Training Accuracy per 5000 steps: 92.53499222395023\n",
      "Training Loss per 5000 steps: 0.2668445833127225\n",
      "Training Accuracy per 5000 steps: 92.54079254079254\n",
      "Training Loss per 5000 steps: 0.26668574181833643\n",
      "Training Accuracy per 5000 steps: 92.54658385093168\n",
      "Training Loss per 5000 steps: 0.26649057530401354\n",
      "Training Accuracy per 5000 steps: 92.55236617532971\n",
      "Training Loss per 5000 steps: 0.2662964153478947\n",
      "Training Accuracy per 5000 steps: 92.55813953488372\n",
      "Training Loss per 5000 steps: 0.26610270243045736\n",
      "Training Accuracy per 5000 steps: 92.56390395042602\n",
      "Training Loss per 5000 steps: 0.26601885211936577\n",
      "Training Accuracy per 5000 steps: 92.56965944272446\n",
      "Training Loss per 5000 steps: 0.26589091812539195\n",
      "Training Accuracy per 5000 steps: 92.5754060324826\n",
      "Training Loss per 5000 steps: 0.26570004877926556\n",
      "Training Accuracy per 5000 steps: 92.58114374034002\n",
      "Training Loss per 5000 steps: 0.2655070736220387\n",
      "Training Accuracy per 5000 steps: 92.58687258687259\n",
      "Training Loss per 5000 steps: 0.26534279797694726\n",
      "Training Accuracy per 5000 steps: 92.5925925925926\n",
      "Training Loss per 5000 steps: 0.26517438049497816\n",
      "Training Accuracy per 5000 steps: 92.59830377794911\n",
      "Training Loss per 5000 steps: 0.26499190439580605\n",
      "Training Accuracy per 5000 steps: 92.6040061633282\n",
      "Training Loss per 5000 steps: 0.26479728740308034\n",
      "Training Accuracy per 5000 steps: 92.60969976905312\n",
      "Training Loss per 5000 steps: 0.26461889133573724\n",
      "Training Accuracy per 5000 steps: 92.61538461538461\n",
      "Training Loss per 5000 steps: 0.26447402471268433\n",
      "Training Accuracy per 5000 steps: 92.62106072252114\n",
      "Training Loss per 5000 steps: 0.26435212971937316\n",
      "Training Accuracy per 5000 steps: 92.62672811059907\n",
      "Training Loss per 5000 steps: 0.2642262274626691\n",
      "Training Accuracy per 5000 steps: 92.63238679969301\n",
      "Training Loss per 5000 steps: 0.26405385268160764\n",
      "Training Accuracy per 5000 steps: 92.63803680981596\n",
      "Training Loss per 5000 steps: 0.2638725614367888\n",
      "Training Accuracy per 5000 steps: 92.64367816091954\n",
      "Training Loss per 5000 steps: 0.2637630035889664\n",
      "Training Accuracy per 5000 steps: 92.64931087289433\n",
      "Training Loss per 5000 steps: 0.26357155077033995\n",
      "Training Accuracy per 5000 steps: 92.65493496557\n",
      "Training Loss per 5000 steps: 0.263382055171529\n",
      "Training Accuracy per 5000 steps: 92.66055045871559\n",
      "Training Loss per 5000 steps: 0.2632373938901881\n",
      "Training Accuracy per 5000 steps: 92.66615737203972\n",
      "Training Loss per 5000 steps: 0.2630639373900667\n",
      "Training Accuracy per 5000 steps: 92.67175572519083\n",
      "Training Loss per 5000 steps: 0.2629179508332234\n",
      "Training Accuracy per 5000 steps: 92.67734553775743\n",
      "Training Loss per 5000 steps: 0.26273261184134045\n",
      "Training Accuracy per 5000 steps: 92.6829268292683\n",
      "Training Loss per 5000 steps: 0.2625385379413056\n",
      "Training Accuracy per 5000 steps: 92.68849961919268\n",
      "Training Loss per 5000 steps: 0.26234909104639764\n",
      "Training Accuracy per 5000 steps: 92.69406392694064\n",
      "Training Loss per 5000 steps: 0.2621686492582691\n",
      "Training Accuracy per 5000 steps: 92.69961977186311\n",
      "Training Loss per 5000 steps: 0.2619898194577375\n",
      "Training Accuracy per 5000 steps: 92.70516717325228\n",
      "Training Loss per 5000 steps: 0.26183286852619825\n",
      "Training Accuracy per 5000 steps: 92.71070615034168\n",
      "Training Loss per 5000 steps: 0.26199519869691956\n",
      "Training Accuracy per 5000 steps: 92.69726858877087\n",
      "Training Loss per 5000 steps: 0.2618064326371848\n",
      "Training Accuracy per 5000 steps: 92.70280515542078\n",
      "Training Loss per 5000 steps: 0.2621567844011059\n",
      "Training Accuracy per 5000 steps: 92.68939393939394\n",
      "Training Loss per 5000 steps: 0.2626040967753796\n",
      "Training Accuracy per 5000 steps: 92.67600302800908\n",
      "Training Loss per 5000 steps: 0.26248587593760303\n",
      "Training Accuracy per 5000 steps: 92.68154311649016\n",
      "Training Loss per 5000 steps: 0.26229758110835033\n",
      "Training Accuracy per 5000 steps: 92.68707482993197\n",
      "Training Loss per 5000 steps: 0.26213536074024896\n",
      "Training Accuracy per 5000 steps: 92.69259818731118\n",
      "Training Loss per 5000 steps: 0.2624444102130409\n",
      "Training Accuracy per 5000 steps: 92.67924528301887\n",
      "Training Loss per 5000 steps: 0.26226850956127745\n",
      "Training Accuracy per 5000 steps: 92.68476621417798\n",
      "Training Loss per 5000 steps: 0.26208141265463675\n",
      "Training Accuracy per 5000 steps: 92.69027882441597\n",
      "Training Loss per 5000 steps: 0.261898350973434\n",
      "Training Accuracy per 5000 steps: 92.69578313253012\n",
      "Training Loss per 5000 steps: 0.26173931065781436\n",
      "Training Accuracy per 5000 steps: 92.7012791572611\n",
      "Training Loss per 5000 steps: 0.2615713515213894\n",
      "Training Accuracy per 5000 steps: 92.70676691729324\n",
      "Training Loss per 5000 steps: 0.2614091478027879\n",
      "Training Accuracy per 5000 steps: 92.7122464312547\n",
      "Training Loss per 5000 steps: 0.26139492832554934\n",
      "Training Accuracy per 5000 steps: 92.69894894894895\n",
      "Training Loss per 5000 steps: 0.26125539001007547\n",
      "Training Accuracy per 5000 steps: 92.70442610652663\n",
      "Training Loss per 5000 steps: 0.2613686168024587\n",
      "Training Accuracy per 5000 steps: 92.6911544227886\n",
      "Training Loss per 5000 steps: 0.2611980617674214\n",
      "Training Accuracy per 5000 steps: 92.69662921348315\n",
      "Training Loss per 5000 steps: 0.2610241277143359\n",
      "Training Accuracy per 5000 steps: 92.70209580838323\n",
      "Training Loss per 5000 steps: 0.26085831877749716\n",
      "Training Accuracy per 5000 steps: 92.70755422587884\n",
      "Training Loss per 5000 steps: 0.26071992207747313\n",
      "Training Accuracy per 5000 steps: 92.71300448430493\n",
      "Training Loss per 5000 steps: 0.2616272824520803\n",
      "Training Accuracy per 5000 steps: 92.68110530246453\n",
      "Training Loss per 5000 steps: 0.2614501122079456\n",
      "Training Accuracy per 5000 steps: 92.68656716417911\n",
      "Training Loss per 5000 steps: 0.2612765470948862\n",
      "Training Accuracy per 5000 steps: 92.69202087994034\n",
      "Training Loss per 5000 steps: 0.2611541120174456\n",
      "Training Accuracy per 5000 steps: 92.69746646795826\n",
      "Training Loss per 5000 steps: 0.26099508321599246\n",
      "Training Accuracy per 5000 steps: 92.70290394638869\n",
      "Training Loss per 5000 steps: 0.2609094594968372\n",
      "Training Accuracy per 5000 steps: 92.70833333333333\n",
      "Training Loss per 5000 steps: 0.2610673417929269\n",
      "Training Accuracy per 5000 steps: 92.69516728624535\n",
      "Training Loss per 5000 steps: 0.2608889450054798\n",
      "Training Accuracy per 5000 steps: 92.70059435364041\n",
      "Training Loss per 5000 steps: 0.26074382507760124\n",
      "Training Accuracy per 5000 steps: 92.70601336302896\n",
      "Training Loss per 5000 steps: 0.2606393732337795\n",
      "Training Accuracy per 5000 steps: 92.71142433234421\n",
      "Training Loss per 5000 steps: 0.26046804309559585\n",
      "Training Accuracy per 5000 steps: 92.71682727946627\n",
      "Training Loss per 5000 steps: 0.2603128706732834\n",
      "Training Accuracy per 5000 steps: 92.72222222222223\n",
      "Training Loss per 5000 steps: 0.2602157314148746\n",
      "Training Accuracy per 5000 steps: 92.72760917838637\n",
      "Training Loss per 5000 steps: 0.26006036307304525\n",
      "Training Accuracy per 5000 steps: 92.73298816568047\n",
      "Training Loss per 5000 steps: 0.26007215307952514\n",
      "Training Accuracy per 5000 steps: 92.71988174427199\n",
      "Training Loss per 5000 steps: 0.26129315349313276\n",
      "Training Accuracy per 5000 steps: 92.68833087149187\n",
      "Training Loss per 5000 steps: 0.2611724143469796\n",
      "Training Accuracy per 5000 steps: 92.69372693726937\n",
      "Training Loss per 5000 steps: 0.2610097216335084\n",
      "Training Accuracy per 5000 steps: 92.69911504424779\n",
      "Training Loss per 5000 steps: 0.26083565841435113\n",
      "Training Accuracy per 5000 steps: 92.7044952100221\n",
      "Training Loss per 5000 steps: 0.26067517294969533\n",
      "Training Accuracy per 5000 steps: 92.7098674521355\n",
      "Training Loss per 5000 steps: 0.26053628891657127\n",
      "Training Accuracy per 5000 steps: 92.71523178807946\n",
      "Training Loss per 5000 steps: 0.2609992655583055\n",
      "Training Accuracy per 5000 steps: 92.70220588235294\n",
      "Training Loss per 5000 steps: 0.2619369890825504\n",
      "Training Accuracy per 5000 steps: 92.68919911829538\n",
      "Training Loss per 5000 steps: 0.26175211225848277\n",
      "Training Accuracy per 5000 steps: 92.69456681350954\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the validation section to print the accuracy and see how it performs')\n",
    "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
    "\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommendation-v2-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
